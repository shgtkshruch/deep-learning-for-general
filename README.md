![](https://github.com/shgtkshruch/deep-learning-for-general/workflows/Build%20README.md/badge.svg)

# Deep Learning for General

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 📜 歴史

##### [ENIAC (エニアック)](https://ja.wikipedia.org/wiki/ENIAC)
1946年にアメリカで開発された世界初のコンピュータ。
17,468本もの真空管を使った巨大な電子計算機。

設計したのはペンシルベニア大学の[ジョン・モークリー](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%BB%E3%83%A2%E3%83%BC%E3%82%AF%E3%83%AA%E3%83%BC)と[ジョン・プレスパー・エッカート](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%BB%E3%83%97%E3%83%AC%E3%82%B9%E3%83%91%E3%83%BC%E3%83%BB%E3%82%A8%E3%83%83%E3%82%AB%E3%83%BC%E3%83%88)
[マーヴィン・ミンスキー](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%BC%E3%83%93%E3%83%B3%E3%83%BB%E3%83%9F%E3%83%B3%E3%82%B9%E3%82%AD%E3%83%BC)、[ジョン・マッカーシー](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%BB%E3%83%9E%E3%83%83%E3%82%AB%E3%83%BC%E3%82%B7%E3%83%BC)、[アレン・ニューウェル](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%AC%E3%83%B3%E3%83%BB%E3%83%8B%E3%83%A5%E3%83%BC%E3%82%A6%E3%82%A7%E3%83%AB)、[ハーバート・サイモン](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%83%BC%E3%83%90%E3%83%BC%E3%83%88%E3%83%BB%E3%82%B5%E3%82%A4%E3%83%A2%E3%83%B3)など、後に人工知能の研究で重要な役割を果たす著名な研究者たちも参加した。

##### [ダートマス会議](http://www.dartmouth.edu/~ai50/homepage.html)

1956年7月から8月にかけて開催された[ジョン・マッカーシー](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%BB%E3%83%9E%E3%83%83%E3%82%AB%E3%83%BC%E3%82%B7%E3%83%BC)が主催した会議。
史上初めて「人工知能（Artificial Intelligence）」という用語が使われた。

##### [ロジック・セオリスト](https://ja.wikipedia.org/wiki/Logic_Theorist)
1955年から1956年にかけてアレン・ニューウェル、ハーバート・サイモン、J・C・ショーが開発した世界初の人工知能プログラム。
コンピュータを使って数学の定理が自動的に証明することが実現可能であることを示した。

## 人工知能のレベル
##### 人工知能 (AI: Artificial Intelligence)
コンピュータによる知的な情報処理システムを設計、または実現するための研究分野。研究者によって定義は異なる。

##### 機械学習
人工知能を実現するための手法のうち特に、人間の学習能力、予測能力をコンピュータで実現しようとする技法や手法の総称

##### ディーブラーニング（深層学習）
ディープニューラルネットワークを用いて学習を行う、機械学習のアルゴリズムの1つ。

##### 特化型人工知能
特定のタスクでのみ成果を出せる人工知能。弱い人工知能。現在の技術ではこちらしか実現できていない。

##### 汎用人工知能
人間と同等の知能を持もった人工知能。強い人工知能。

##### レベル1 (マーケティング用 AI)
「人工知能搭載X」のような家電など

##### レベル2 (弱い AI)
チェスプログラム、将棋マシーン

##### レベル3 (強い AI)
人間の知能を目指そうとするもの

##### レベル4 (強い AI を超えうるもの)
表現学習、深層学習

##### AI効果
人工知能で何か新しいことを実現したときに、その原理がわかってしまうと、「それは単純な自動化であって知能とは関係ない」と結論づける人間の心理的効果

##### [ELIZA効果](https://ja.wikipedia.org/wiki/ELIZA%E5%8A%B9%E6%9E%9C)
人間は、相手が高度な知能を持った存在ではなくとも、自分に適切に反応してくれれば、本物の人間と対話しているように錯覚する傾向があること

## 第1次AIブーム（1956 - 1974: 探索・推論による人工知能）

冷戦下のアメリカで、自然言語処理による機械翻訳の研究に注力されていたことが有名。しかし当時は、実用的な機械翻訳を行うことはきわめて困難であると結論された。

##### [チューリングテスト](https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A5%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%83%BB%E3%83%86%E3%82%B9%E3%83%88)(1950)
ある機械が人工知能かどうか判定するためのテスト。「機械が知能をもっているか」という問いから、「機械が知能をもっている存在として人間が認知できるか」という問題に置き換えている。

##### 推論
自分がもつ知識と知識を組み合わせることで新しい知識を見つけ出す

##### 探索
推論により導き出せる結果を、いかに早くおこなえるかを求めた手法。問題をうまく表現することができれば、効率的に解を見つけ出すことができる

##### 人工知能を考える視点
環境・状態・行動をコード化できるとそのタスクは人工知能で解くことができる。推論・探索ではこの3要素を機械が理解できる形に書けないと解くことができない。

##### トイ・プロブレム
迷路やオセロなど、機械にときやすい簡単な問題

##### [ELIZA (イライザ)](https://ja.wikipedia.org/wiki/ELIZA)
1966年に発表された自然言語処理プログラム。[人工無脳](https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%84%A1%E8%84%B3)の起源となった。

##### [PARRY](https://ja.wikipedia.org/wiki/PARRY)
1972年に開発された ELIZA と共に有名な初期の会話ボット。ELIZA との最初の会話記録は [RFC439](https://tools.ietf.org/html/rfc439) に残されている。

##### [BFS (幅優先探索)](https://ja.wikipedia.org/wiki/%E5%B9%85%E5%84%AA%E5%85%88%E6%8E%A2%E7%B4%A2)
隣接するノードを優先して探索するアルゴリズム

##### [DFS (深さ優先探索)](https://ja.wikipedia.org/wiki/%E6%B7%B1%E3%81%95%E5%84%AA%E5%85%88%E6%8E%A2%E7%B4%A2)
目的のノードが見つかるか子のないノードに行き着くまで、深く探索するアルゴリズム

##### ハノイの塔

##### ロボットの行動計画（プランニング）

##### STRIPS

##### SHRDLU

##### [Mini-Max法](https://ja.wikipedia.org/wiki/%E3%83%9F%E3%83%8B%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E6%B3%95)
想定される最大の損害が最小になるように決断を行う戦略

##### [モンテカルロ法](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95)
シミュレーションや数値計算を乱数を用いて行う手法の総称。ランダム法とも呼ばれる。

## 第2次AIブーム（1980 - 1987: 知識表現による人工知能）

エキスパートシステムにより問題を解く人工知能が台頭。しかし専門家の知識の定式化は難しく、複雑な問題が解けるようにならなかった。

##### エキスパートシステム
専門家の知識をそのまま人工知能に移植する事により、さまざまな問題を解決するアイディア

##### [MYCIN (マイシン)](https://ja.wikipedia.org/wiki/Mycin)
1970年代初めに開発された抗生物質を処方するAI

##### [DENDRAL](https://ja.wikipedia.org/wiki/Dendral)
1960年代の[エドワード・ファイゲンバウム](https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%89%E3%83%AF%E3%83%BC%E3%83%89%E3%83%BB%E3%83%95%E3%82%A1%E3%82%A4%E3%82%B2%E3%83%B3%E3%83%90%E3%82%A6%E3%83%A0)が開発した未知の有機化合物の特定するエキスパートシステム

##### 知識ベース
if-then 文よって記述できる知識の集まり

##### 推論エンジン
知識ベースを用いて推論を行うプログラム

##### 第五世代コンピュータ
通商産業省（現経済産業省）が1982年に立ち上げた国家プロジェクト。

[第五世代コンピュータ・プロジェクト最終評価報告書](https://www.jipdec.or.jp/archives/publications/J0005062)（平成5年3月30日 電子計算機基礎技術開発推進委員会）

##### 人工無能
会話ボットやチャットボットなど、主にテキストを用いた会話をシミュレートするコンピュータプログラム

##### 意味ネットワーク
もともと認知心理学における長期記憶の構造モデルとして考案されたもの。
現在は、人工知能においても重要な知識表現の一つ。

「概念」をラベルの付いたノードで表し、概念間の関係をラベルの付いたリンク（矢印）で結んだネットワークで表す。

  - is-a 関係

  上位概念と下位概念の関係を表す。継承関係（ex. 動物は生物である。哺乳類は動物である。）

  - part-of 関係

  全体と部分の関係を表す。属性（ex. 目は頭部の一部である。肉球は足の一部である。）

##### [Cycプロジェクト](https://ja.wikipedia.org/wiki/Cyc%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88)
1984年にスタートした、すべての一般常識をコンピュータに取り込むプロジェクト

##### オントロジー
AI におけるオントロジーとは「概念化の明示的な仕様」（Tom Gruber）であり、共通の概念の体系（語彙とその定義）とそれらの関係のことを指す。

本体は哲学用語で「存在論」という意味だが、人工知能の用語としては、[トム・グルーバー](https://en.wikipedia.org/wiki/Tom_Gruber)による「概念の明示的な仕様」という定義が広く受け入れられている。

##### ヘビーウェイトオントロジー
対象世界の知識をどのように記述すべきかを哲学的にしっかり考えて行うもの。

哲学的な考察が必要になるため、人間が関わる傾向が強く、時間とコストがかかる（ex. Cycプロジェクト）

##### ライトウェイトオントロジー
効率を重視し、とにかくコンピュータにデータを読み込ませてできる限り自動的に行うもの。

完全に正しいものでなくても使えるものであればいいという考えから、その構成要素の分類関係の正当性については深く考察は行わない傾向にある。
コンピュータで概念間の関係性を自動でみつける取り組みがある。（ex. ウェブマイニング、データマイニング、ワトソン）

##### [ワトソン](https://ja.wikipedia.org/wiki/%E3%83%AF%E3%83%88%E3%82%BD%E3%83%B3_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF))
2011年にアメリカのクイズ番組で歴代の人間チャンピオンに勝利した IBM が開発した質問応答システム。
ウィキペディアの情報をもとにライトウェイト・オントロジーを生成して、それを解答につかっている。


## 第3次AIブーム（2000 -: 機械学習と深層学習による人工知能）

##### 機械学習

##### ディーブラーニング

##### 特徴量
データをよく表す特徴を数値で示したもの（ex. 「人間」を表す特徴量としては、身長、体重、年齢、性別など）

##### 特徴量エンジニアリング
モデルが認識しやすいような「特徴」をデータから新しく作ること。

たった1つの成分だけが 1、残りの成分が 0 という特徴量（ベクトル）に変換する。この形のことを [one-hot-encoding](https://ja.wikipedia.org/wiki/One-hot) と呼ぶ。

##### 内部表現
ディープラーニングにより自動的に獲得された特徴量。

##### [ILSVRC (Large Scale Visual Recognition Challenge)](http://www.image-net.org/challenges/LSVRC/)
2010年から始まった ImageNet データセットを用いた画像認識の精度を競うコンペ。

2012年、トロント大学の[ジェフリー・ヒントン](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%95%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%92%E3%83%B3%E3%83%88%E3%83%B3)教授のチームが [AlexNet](https://en.wikipedia.org/wiki/AlexNet) と呼ばれる畳み込みニューラルネットワーク(CNN)で[2位以下を10%上回る正答率](http://image-net.org/challenges/LSVRC/2012/results.html)を出す。これがきっかけとなり、ディーブラーニングが脚光を浴びる。

[人工知能](https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD) > [機械学習](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92) > [ニューラルネットワーク](https://ja.wikipedia.org/wiki/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF) > [ディープラーニング](https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0)

##### [モラベックのパラドックス](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%A9%E3%83%99%E3%83%83%E3%82%AF%E3%81%AE%E3%83%91%E3%83%A9%E3%83%89%E3%83%83%E3%82%AF%E3%82%B9)
子供のできることほど人工知能には難しい

### ILSVRC (Image Large Scale Visual Recognition Challenge)
2010年から始まった大規模画像認識の競技会。

[ImageNet](http://www.image-net.org/index) は、画像に写っている物体名（クラス名）を付与したデータベース。

[Overview - ImageNet](http://www.image-net.org/about-overview)

#### 2012年

ディープラーニングが脚光を浴びるきっかけになった年。

結果: http://image-net.org/challenges/LSVRC/2012/results

##### AlexNet

畳み込み層とプーリング層を深くしていく構造


#### 2014年

層を深くしていくと計算量が非常に大きくなって学習が進まなくなるおちう問題があったが、小さなサイズの畳み込みフィルター（1x1, 3x3）を差し込んで次元（計算量）を削減するという工夫が取られるようになった。

結果: http://image-net.org/challenges/LSVRC/2014/results

##### GoogLeNet
2014年のコンペで1位になったアーキテクチャ。

このアーキテクチャは通常の入力層から出力層まで縦一直線な構造ではなく、インセプション構造と呼ばれる横にも層が広がる構成にすることで、並列計算を行いやすくしている。このため、Inceptionモデルとも呼ばれる。

ref: https://arxiv.org/pdf/1409.4842.pdf

##### VGG
2014年のILSVRCで2位になった、オックスフォード大学のVGGチームのネットワーク

#### 2015年

結果: http://image-net.org/challenges/LSVRC/2015/results

##### ResNet (Residual Network)
2015年のILSVRCで優勝したネットワーク。

それまでのネットワークでは層を深くしすぎると性能が落ちるという問題があったが、それを「スキップ構造」によって解決し、152層もの深さ(前年優勝のGoogLeNetでも22層)を実現した。

以下のような理由で学習がうまくいっている。
- 層が深くなっても、層を飛び越える部分は伝播しやすくなる
- 様々な形のネットワークのアンサンブル学習になっている

ref: https://arxiv.org/pdf/1512.03385v1.pdf

# 👻 人工知能分野の問題

##### [シンギュラリティー（技術的特異点）](https://ja.wikipedia.org/wiki/%E6%8A%80%E8%A1%93%E7%9A%84%E7%89%B9%E7%95%B0%E7%82%B9)
[レイ・カーツワイル](https://ja.wikipedia.org/wiki/%E3%83%AC%E3%82%A4%E3%83%BB%E3%82%AB%E3%83%BC%E3%83%84%E3%83%AF%E3%82%A4%E3%83%AB) が提唱した、人工知能が人間を超えて文明の主役に取って代わる時点。

カーツワイルは自著「The Singularity Is Near」で「シンギュラリティは2045年に到来する」と述べた。

- スティーブン・ホーキング

  「完全な人工知能を開発できたら、それは人類の終焉を意味するかもしれない」

- イーロン・マスク

  「人工知能はかなり慎重に取り組む必要がある。結果的に悪魔を呼び出すことになるからだ。ペンラグラムと聖水を手にした少年が悪魔に立ち向かう話を皆さんもご存知だろう。少年は必ず悪魔を支配できると思っているが、結局はできはしないのだ」

- ビル・ゲイツ

  「私も人工知能に懸念を抱く側にいる1人だ」

##### トイ・プロブレム

##### フレーム問題
ある問題を解決する際に、その問題に関連する、考慮すべき事柄を抽出することが AI には難しいこと

##### 強いAI
フレーム問題を打破し、人間のようにあらゆる問題に適切に対処できるようになった AI。

哲学者の[ジョン・サール](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%BB%E3%82%B5%E3%83%BC%E3%83%AB)が1980年に発表した論文の中で提示した区分。

##### 弱いAI
フレーム問題に縛られたままの AI

##### シンボルグラウディング問題
記号システム内のシンボルがどのようにして実世界の意味と結び付けられるかという問題

##### [He saw a woman in the garden with a telescope](https://www.deepl.com/ja/translator#en/ja/He%20saw%20a%20woman%20in%20the%20garden%20with%20a%20telescope)

##### 特徴量設計
機械学習において、正しい特徴量を見つけるのは一般に非常に難しいタスク。
人間が特徴量を見つけるのが難しいのであれば、特徴量を機械学習自身に発見させれば良い。
このアプローチは「**特徴表現学習**」と呼ばれ、ディープラーニングはこの「特徴表現学習」を行う機械学習アルゴリズムの一つ。

しかし、コンピュータが自動的に特徴量を抽出するため、特徴量が意味することを本当の意味で理解することはできない。（ブラックボックス）

# 🤖 機械学習

世の中の特定の事象について**データを解析し**、その結果から学習し、**判断や予測を行う**ためのアルゴリズムを使用する手法

機械学習はデータが命。データから答えを探し、データからパターンを見つけ、データからストーリーを語る。
機械学習の中心には「データ」があり、このデータ駆動によるアプローチは、「人」を中心とするアプローチからの脱却とも言える。
重みパラメータの値をデータから自動で計算できる。（ディープラーニング）

## 処理の流れ
機械学習の処理は大きく「学習」と「推論」ステップに分かれる。

##### 学習
事前に与えられたデータからモデルをつくること。
学習が終わったモデルには、学習データの統計的な傾向や規則性が反映されている。

##### 推論
実環境で得られるデータなどを使い、学習済みモデルを使って判定、分類、予測などを行う。

## 特徴量

機械学習の予測モデルに入力する情報。

例えば明日の積雪のウムを予測するために、今日の気温（1.0℃)、降水量(0.8mm)、天気（曇り）使うとすると、それぞれを数値化したもののリスト（ex. [1, 0.8, 1]) が「特徴ベクトル」になります。

ここで、「曇り」のような特徴量を「カテゴリカル変数」と呼び、晴れは 0、曇は 1 というような数値データのことを「ダミー変数」と呼ぶ。

## 代表的な手法

![](https://jp.mathworks.com/discovery/machine-learning/_jcr_content/mainParsys3/discoverysubsection_1965078453/mainParsys3/image_2109075398_cop.adapt.full.high.svg/1591624225922.svg)

[機械学習とは？これだけは知っておきたい3つのこと - MATLAB & Simulink](https://jp.mathworks.com/discovery/machine-learning.html)

##### 代表的な「予測」アルゴリズム

| アルゴリズム | 回帰 | 分類 | クラスタリング |
| :---: | :---: | :---: | :---: |
| 線形回帰 | o | x | x |
| ロジスティック回帰 | x | o | x |
| サポートベクターマシン | o | o | x |
| ナイーブベイズ | x | o | x |
| 決定木 | o | o | x |
| ランダムフォレスト | o | o | x |
| ニューラルネットワーク | o | o | x |
| kNN | o | o | x |
| k-means | x | x | o |

##### scikit-learnのアルゴリズム・チートシート
![](https://scikit-learn.org/stable/_static/ml_map.png)

[Choosing the right estimator — scikit-learn 0.23.1 documentation](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

##### 教師あり学習
教師データ（入力とそれに対応する正解ラベルの組）を使って予測値を正解ラベルに近づけることを目標に学習する手法
自然言語処理、スパムメールフィルタリング、手書き文字認識などの応用がある。

##### 教師なし学習
教師データを使わずに、データの本質的な構造を浮かび上がらせる手法。
クラスタリングやデータ次元圧縮技術などがこれに含まれる。

##### 強化学習
ゴールや目標を仮定せず、学習を行う「エージェント」が状態を観察し、環境からの報酬を最大化するように試行錯誤しながら行動を選択することで学習を行う。
ロボット操作などに使われることが多い。

##### 転移学習
すでにある領域で学習済みのモデルを他の領域に流用する手法。

##### 半教師あり学習
教師あり学習におけるラベル付けコストを低減するために用いられる手法。
データの一部分にのみ正解ラベルをつける。

##### 回帰問題
正解となる数値を入力データの組み合わせで学習し、未知のデータから連続値を予測する。

統計学において、目的変数と説明変数の関係を明らかにすることで、未知のデータ属性の値を既知のデータ属性から予測できる。
（ex. 家賃の高さを広さ、築年数、駅からの近さの値から計算）

入力データが1つの単回帰分析、複数の入力データの重回帰分析などの種類がある。

##### 分類問題
正解となる離散的なカテゴリ（クラス）と入力データの組み合わせ出学h数詞、未知のデータからクラスを予測する。

あらかじめ設定したクラスにデータを割り振る（ex. 画像に写っている犬や猫の識別）。
サポートベクターマシン、決定木、ランダムフォレスト、ロジスティック回帰、kNN法などの手法がある。深層学習でも分類を行うことができる。

##### クラスタリング
データを何かしらの基準でグルーピングする。
分類と違い、事前にクラスなどの分類の軸が提供されないため、与えられたデータの特徴などから自動的にクラスタを構成する。

事前学習が不要なため、過去のデータがない場合でもクラスタ化可能だが、クラスに基づく分類とは異なる結果がでる場合がある。
代表的なアルゴリズムに k-means法などがある。

##### 次元削減
高次元のデータを可視化や計算量削減などのために低次元にマッピングする。

## データの前処理

データをモデルに正しく入力できるようにする。
データの大きさをある程度均一にする。

### 正規化

##### 離散化
![](https://miro.medium.com/max/2000/1*LGTAObYYj2-fdBMFLz30rw.jpeg)
https://heartbeat.fritz.ai/hands-on-with-feature-engineering-techniques-variable-discretization-7deb6a5c6e27

連続した値をある区分にわけること。

##### 対数変換
![](https://www.researchgate.net/profile/Matthieu_Komorowski4/publication/308007227/figure/fig5/AS:405478883512321@1473685107077/[55-Example-of-the-effect-of-a-log-transformation-on-the-distribution-of-the-dataset.png)
https://www.researchgate.net/figure/55-Example-of-the-effect-of-a-log-transformation-on-the-distribution-of-the-dataset_fig5_308007227

値の log（対数）を取る（log に変換する）こと。

正の値をもつ数値データにおいて、長い裾を短く圧縮し、小さい値を拡大することができる。
機械学習では正規分布に近いデータが効果を発揮しやすいため、対数変換は有効な手段の一つ。

##### スケーリング
![](https://kharshit.github.io/img/scaling.png)

https://kharshit.github.io/blog/2018/03/23/scaling-vs-normalization

データを 0 から 1 に収まるようにスケーリングすること。

代表的なスケーリングの方法に「Min-Maxスケーリング」と「標準化」がある。

- Min-Maxスケーリング

  最小値を 0、最大値を 1 にし、データの範囲を 0 ~ 1 に変換する。

- 標準化

  平均を 0、標準偏差（分散）を 1 に変換する。
  対数変換をしてから標準化を行う場合もある。

##### 白色化　　
各特徴量を無相関化した上で標準化する手法。
白色化は計算コストが高いので、標準化を用いるのが一般的。

##### 基礎集計
データの傾向を事前に把握する。散布図行列をプロットして傾向を調べる。相関行列を表示し傾向を調べる。


## 過学習を抑制する手法

##### 正則化
線形回帰式で利用可能な手法。

学習に用いる式に項を追加することによってとりうる重みの値の範囲を制限し、過度に重みが訓練データに対してのみ調整されることを防ぐ。

（回帰）係数を大きくなりすぎないように自動で調整することで、予測結果を安定化させる。

誤差関数にパラメータのノルムによる正則化（LASSOなど）を付け加える。

##### L1正則化
一部のパラメータの値をゼロにすることで、特徴選択を行うことができる。
線形回帰に対してL1正則化を適応した手法を「[ラッソ回帰](https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%83%E3%82%BD%E5%9B%9E%E5%B8%B0)」という。

##### L2正則化
パラメータの大きさに応じてゼロに近づけることで、汎化されたなめらかなモデルを得ることができる。
線形回帰に対してL2正則化を適応した手法を「リッジ回帰」という。

##### [Elastic Net](https://jp.mathworks.com/help/stats/lasso-and-elastic-net.html)
ラッソ回帰とリッジ回帰の両者を組み合わせた手法。


## 教師あり学習

### パーセプトロン

入力ベクトルと学習した重みベクトルを掛け合わせた値を足して、その値が0以上のときはクラス1, 0未満のときはクラス2と分類するという、シンプルなアルゴリズム。

過学習しやすく、線形分離可能な問題のみ解ける。

### 回帰分析

簡単にいうと「データにもっともフィットする線を引くこと」

#### 単回帰分析
1つの説明変数（手がかりとなる変数）から目的変数を予測する。
データと回帰直線との最小二乗法が誤差関数（損失関数）。

※ 説明変数は文脈にとっては特徴量と呼ばれることがある。特徴量の方が少し大きな概念になるが、同じものを指していると考えていい。

#### 重回帰分析
複数の説明変数から目的変数を予測する。
多重共線性に注意する必要がある。

##### 多重共線性
相関係数が高い特徴量の組を同時に説明変数に選ぶと、予測がうまくいかなくなる現象。

多重共線性を避けるには、相関の強い説明変数のどちらか一方を除く。
特徴量エンジニアリングにおいては、多重共線性がでないような特徴量を選ぶ必要がある。

### 用語

##### 相関係数
特徴量同士の相関の正負と強さを表す指標。-1 <= x <= 1。
1に近いほど強い正の相関、-1に近いほど強い負の相関をもつ。0のときは相関がない。

##### 線形分離可能
パーセプトロンを使って解ける問題。2次元のグラフ上で直線を使って分離できる。
論理ゲートでは OR, AND, NAND は線形分離可能だが、XOR のみ線形分離不可能。

### SVM（サポートベクターマシン）

データを最も引き離す境界線を引くための手法で、ディープラーニングを使わない機械学習の中では主流の方法。

##### [SVM (サポートベクターマシン)](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3)
「マージン最大化」というコンセプトで2つのクラスを線形分離可能にするアルゴリズム。

もともとは2つのクラス分類アルゴリズムとして考案されたが、性能がいいので多クラス分類や回帰分析にも応用されている。

線形分離可能でないデータに対しても、カーネル法を組み合わせることで決定境界を求めることができる。
未学習のデータに対しても高い識別性能があるが、仮説の設定や特徴の選択が必要。

##### [スラック変数](http://sudillap.hatenablog.com/entry/2013/04/08/235602)
超平面を構成した結果として発生する誤差の程度を測る変数。

##### [ハイパーパラメータ](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF)
推論や予測の枠組みの中で決定されないパラメータ。
SVM においては誤りをどの程度許容するかの度合いをエンジニアが事前に調整する必要がある。

##### [カーネル法](https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E6%B3%95)
派閥の境界が非線形になっている場合に用いる手法。

データから新しい特徴量をつくって、線形分離可能になるようにプロットしなおす。

次のアイディアをカーネル関数の計算によって実現するもの。
- データを高次元空間にうまく埋め込む
- 高次元空間で線形分離 (SVM) し、その境界を元の空間に戻す

##### カーネルトリック
カーネル関数を使って、計算複雑度の増大を抑えつつ内積にもとづく解析手法を高次元特徴空間へ拡張するアプローチ。
計算量を現実的に抑えつつ、非線形分離を実現する。

### 決定木
![](https://s3-ap-southeast-1.amazonaws.com/he-public-data/XOR%203df51ae5.png)

https://www.hackerearth.com/ja/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/

Yes or No で答えられる条件によって予測を行う手法。

人間の思考プロセスに近い方法のため、結果がわかりやすいのが特徴。

##### 決定木
性質（ex. 男女）や数値（ex. 購入数5個以上/未満）に基づき、データを木構造に分類する手法。予測にも使える。

人が理解しやすく前処理が少なくてすむが、データに対する条件分岐が複雑になりやすく、過学習を起こしやすい。

1. 不純度が最も減少（情報利得が最も増加）するように、条件分岐を作りデータを振り分ける
1. それを繰り返す

不純度とは「クラスの混じり具合」を表す指標で、代表的なものにジニ係数やエントロピーがある。

### アンサンブル学習

学習器を複数組み合わせて1つの学習モデルを生成する方法。

「三人集まれば文殊の知恵」を機械学習で実現する方法ともいえるかも。

##### アンサンブル学習
複数の学習モデルを組み合わせて1つの学習モデルを生成する手法。

弱学習器を使って学習を行うため、学習スピードが早く、訓練や予測にかかる時間が少なくて済む。
最終的な予測結果は「多数決」「平均」「加重平均」といった方法で求める。

#### 手法

アンサンブル学習の手法は大きく分けて以下の2つの手法がある。

##### バギング
ブートストラップ法を使って、全データから訓練データを複数組生成し、訓練データ1組1組に対してモデルを用意して並列して学習を行う。
各モデルの結果平均をとって予測結果とする。

##### ブースティング
以下のフローで各モデルを逐次的に学習させる手法。

1. 訓練データ1つ目のモデルに学習させ、予測結果と実際の値を比較する
1. 次のモデルを学習する際に、間違えた部分を正解にできるように学習したデータを重視して学習する

ブースティングには以下のようなアルゴリズムがある。
##### [AdaBoost](https://ja.wikipedia.org/wiki/AdaBoost)
##### 勾配ブースティング
##### XGBoost
勾配ブースティング法を使ったライブラリ。
確率的な最適化処理をしているため大規模なデータでも高速に処理できる。

#### アンサンブル学習の応用
##### ランダムフォレスト
ランダムに選んだ学習データを説明変数を用いて決定木群を作成し、その多数決や平均値を結果として出力する。

決定木にバギングを組み合わせていて、以下のような利点があり、非常によく使われている。
- データの前処理が少なくて済む
- 安定して良い精度が出る

一方で、中身がブラックボックス化するという面もある。

##### スタッキング
前段階のモデルの予測結果を学習するため、データの偏り（バイアス）とデータの散らばり（バランス）を上手に調整できる。

1. バギングと同じように、ブートストラップ法で得たデータを各モデルに学習させ、各モデルの予測結果を出す
1. 上記予測結果を入力としてモデルを学習させる
1. 以降も同様に前段階の予測結果を入力として学習する

##### 勾配ブースティング
サンプリングしたデータに対して直列的に浅い決定木を学習していく手法。
前モデルの誤差を取ることによって、新しいモデルが古いモデルの欠点を穴埋めをする。

1. 決定木を用いて1回目の予測を行う
1. 訓練セットの正解データと予測結果の差をとり、誤差を算出する
1. この誤差を正解データとして、決定木を使って2回目の予測を行う
1. 上記を繰り返す 

### ロジスティック回帰

「回帰」という言葉がついているが、主に分類に使われるアルゴリズム。

Yes / No の確率を計算するさまざまな場面で利用される。

##### [ロジスティク回帰](https://ja.wikipedia.org/wiki/%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0)
AかBどちらかしか起こらない確率などに、事象がどちらかに分類できるかの確率を計算する。

線形回帰を分類問題に応用したアルゴリズム。（線形分離可能な対象を分離するアルゴリズム）

1. 「対数オッズ」を重回帰分析により予測する
2. 「対数オッズ」をロジスティック関数（シグモイド関数）で変換することで、クラスiに属する確率piを求める
3. 各クラスに属する確率を計算し、最大確率を実現するクラスが、データの所属するクラスと予測する

また、「ロジット変換」を行うことで、出力値が 0 から 1 の間の値に正規化され、確率としての解釈が可能になる。

### ベイジアンモデル

ベイズ推定を用いて、不確実性を考慮した予測を行うことができる。

##### ベイズ推定
確率の初期状態（事前確率）に対して、得られたデータにより確率（事後確率）を計算（ベイズ更新）して推定を行う。
推定結果（値）がどれだけふさわしいかがわかる。

データを増やすことで精度を向上できるが、学習データで仮定した確率分布以外では未対応。

### KNN法

##### [kNN法 (k-nearest neighbor)](https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95)
クラス分類のアルゴリズムの一種。 

データから近い順に k 個のデータを見て、それらの多数決によって所属クラスを決定する。
kNN法は柔軟にモデルを作れるが、実用上、以下の条件がないと精度が上がらない弱点がある。

- 各クラスのデータ数に偏りがない
- 各クラスがよく別れている

## 教師なし学習

代表的な手法は、クラスタリングと次元分析。

##### クラスタリング
データ群をいくつかの集まり（クラスタ）に分けることで、データの本質的な構造を浮かび上がらせる手法。
クラスタはデータから自動的に導かれる。

##### クラス分類
エンジニアが予め設定した「クラス」にデータを適切に分類する（教師データを使う）

##### 主成分分析
多次元のデータに対して正味に効果のあるより少ない成分を抽出（次元の削減）する手法。

データから「主成分」という新たなデータを求める、線形な次元削減の手法。
変数間に相関のないデータに対しては有効でない。

##### 次元圧縮
データの情報を失わないようにデータを低い次元に圧縮する手法の総称。（ex. 身長と体重から肥満度を表す BMI を計算する）

##### [k-means (k平均法)](https://www.albert2005.co.jp/knowledge/data_mining/cluster/non-hierarchical_clustering)
異なる性質のものが混ざり合った集団から、互いに似た性質を持つものを集め、クラスターを作る方法の1つ。

あらかじめいくつのクラスターに分けるかを決め、決めた数の塊（排他的部分集合）にサンプルを分割する。
手法が理解しやすく、大規模なデータにも適応可能。

## 手法の評価

機械学習において重要なことは、データを学習することで未知のデータの予測や分類を行えるようにする（汎化性能）こと。

学習が終わった段階では未知のデータに対する性能が保証されていないので、汎化性能を検証する必要がある。

##### 教師データ
区別がまぎらわしい少数のデータのラベルを作成して学習したほうが精度があがる。

##### 訓練データ
学習に用いる分の教師データ。

##### 精度検証データ（validation data）
「モデルの評価」で使うチューニング用データ。

このデータを使って、正解ラベルと一致するかをチェックすることで、「対象の機械学習モデルがどのくらいの精度が出せるのか」というパフォーマンス（性能）を評価・検証する。

##### テストデータ
未知データへの予測性能（汎化性能）を測るための教師データ。

##### 過学習
学習済みのモデルが、訓練に対しては高い精度で正解ラベルを予測できる（訓練誤差が小さい）にもかかわらず、未知のデータに対しての予測精度が悪いままになってしまう現象。
機械学習における最大の問題？と呼ばれている。

##### 推定誤差 (Validation Loss)
未知のデータ（テストデータ）とモデルとの間に生じた誤差のこと。

##### 訓練誤差
出力データと正答データの間に生じた誤差。

### 学習データとテストデータを分ける手法

##### ホールドアウト検証
データを訓練データとテストデータにある割合で分割して、モデルが過学習を起こしていないか調べるための手法。

**モデルを評価するためのデータ**
- テストデータ

  やがて手に入るであろう、正解ラベルがあるとは限らないデータ。

- 検証データ

  あらかじめ手元にあり、正解ラベルがあることが約束されているデータ。

##### [K-flod クロスバリデーション（K-分割交差検証）](https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC)
テストデータに用いるブロックを順に移動しながらホールドアウト法による検証を行う。

計算量が多くなるなど欠点があるが、データ数が少ない場合にもホールドアウト法と比較して信頼できる精度が増えるなどの理由で、精度検証において最もよく用いられる。

### 学習結果に対する評価基準

#### 回帰
回帰モデルの性能は基本的に、出力と正答の数値の差分である「予測誤差」によって評価できる。

回帰モデルの評価指標の違いは、**予測誤差をどのように集計するか**の違い。

##### 決定係数
予測誤差を正規化することで得られる指標。
まったく予測できていない場合を 0、すべて予測できている場合を 1 として大きいほど性能が良いことを示す。

##### RMSE（平方平均二乗誤差）
予測誤差を二乗して平均したあとに集計する指標。
小さいほど性能が良いこと示す。
正規分布の誤差に対して正確な評価ができるため、多くのケースで使われている。

##### MAE（平均絶対誤差）
予測誤差の絶対値を平均したあとに集計する指標。
小さいほど性能が良いことを示す。
RMSE と比較して外れ値に強いため、多くの外れ値が存在するデータセットで評価する場合に利用される。

#### 分類

##### 混同行列

| | 正解が「o」| 正解が「x」|
| :---: | :---: | :---: |
|「o」と予想 | TP | FP |
|「x」と予想 | FN | TN |

##### 正解率（Accuracy）
`(TP + TN) / (TP + TF + FN + TN)`

全体のデータのうち正しく分類できたデータの数の割合。

##### 再現率 (Recall)
`TP / (TP + FN)`

実際に正であるものの中で、正だと予測できたデータの割合。

「絶対にミスをしてはいけない」などのケースでは重視される指標。（ex. 医療検診）

##### 適合率 (Precision)
`TP / (TP + FP)`

予測が正の中で、実際に正だったデータの割合。

「顧客の好みでない商品を提案したくない」などのケースでは重視される指標。（ex. WEBマーケティング）

##### F値 (f-score)
`(2 x Recall x Precision) / (Recall + Precision)`

適合率と再現率の調和平均。
適合率のみあるいは再現率のみで判断すると、予測が偏っているときも値が高くなってしまうので、F値を用いることも多い。

適合率と再現率のバランスが見れる。機械学習モデルの評価の際に、正解率と並んで最も使われる指標。

##### PR曲線
横軸を再現率(Recall)、縦軸を適合率/精度(Precision)として、データをプロットしたグラフを表したもの。

PR曲線には適合率と再現率が一致する点があり、この点を「ブレークイーブンポイント(BEP)」と呼ぶ。
この点では、適合率と再現率の関係をバランスよく保ったまま、コストと利益を最適化できるので、ビジネスにおいては重要な点となる。BEPが右上に遷移するほど良いモデルが構築できたと言える。

## 用語

##### [No Free Lunch（ノーフリーランチ）定理](https://ja.wikipedia.org/wiki/%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86)
一つの学習済みモデルで様々用途に対応できるわけではないこと数学的に証明した定理。

##### [みにくいアヒルの子定理](https://dic.nicovideo.jp/a/%E3%81%BF%E3%81%AB%E3%81%8F%E3%81%84%E3%82%A2%E3%83%92%E3%83%AB%E3%81%AE%E5%AD%90%E3%81%AE%E5%AE%9A%E7%90%86)
1969年に情報理論学者・理論物理学者の[渡辺慧](https://ja.wikipedia.org/wiki/%E6%B8%A1%E8%BE%BA%E6%85%A7)が提唱した定理。

対象がもつ特徴をすべて同等に評価すると識別が困難になること。
機械学習で行われる「特徴選択」や「次元削減」といった処理が人間の主観的な特徴選択と同様であり、識別にとって本質的であることも示している。

# 🦄 ディープラーニング

ディープニューラルネットワークを用いた機械学習の手法。
2010年代に脚光を浴びたが、アルゴリズム自体は1960年代にはすでに考案されていた。 

ニューラルネットワークを3層より多層にしても学習精度が上がらない壁にぶつかったが、自己符号化器の研究などを足場にして、層を深くしても学習することが可能となった。

人工知能の大家[マービン・ミンスキー](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%BC%E3%83%93%E3%83%B3%E3%83%BB%E3%83%9F%E3%83%B3%E3%82%B9%E3%82%AD%E3%83%BC)によって、特定の条件下の単純パーセプトロンでは直線で分離できるような単純な問題しか解けないということが指摘されたが、バックプロパゲーション（誤差逆伝播法）で克服できることが示された。

「深い関数を使った最小二乗法」

「シグモイド関数など活性化関数を使って非線形性を入れ、多層に構成した関数を使った、損失関数の最小化」

## 変遷
##### 1958年
パーセプトロン

##### 1969年
パーセプトロンの性能と限界に関する論文
（冬の時代が始まる）

##### 1986年
バックプロパゲーション（誤差逆伝播法）
（第二次 AI ブーム）

##### 2006年
自己符号化器
（第3次 AI プームのきかっけ）

##### 2012年
ILSVRC でトロント大学の SuperVision が優勝

##### 2017年
AlphaGo が人間のプロ囲碁棋士を破る

## ニューラルネットワーク

人間の神経回路（ニューロン）の構造をモデル化したネットワーク。

##### ニューロン
単純な数値予測ができ予測器。ニューラルネットワークの最小単位。
重みが乗算された入力を受け取り、それらを総和して出力する。

##### ニューラルネットワーク
ニューロンをたくさんつなげててきる予測器。入力が行わる層を「入力層」、出力される層を「出力層」、それ以外の層を「中間層」または「隠れ層」と呼ぶ。

また、入力層の各ノードが隠れ層のすべてのノードと結合しているような層を「全結合層」と呼ぶ。
人間の脳を模倣したモデルではなく、「人間の脳の仕組みの一部」を模倣している。

##### [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3)
1982年に福島邦彦氏によって発表された、畳み込みニューラルネットワークの発想の基となったネットワークモデル。

##### 宝くじ仮説
「ランダムに初期化された密なニューラルネットワークは、たまたまうまく学習ができるように初期化されたサブネットワークが含まれており、このサブネットは、学習が進むにつれて他のサブネットよりも優れているので早く学習が進み、他の劣るサブネットの活性が抑制される」という仮説。

この仮説を提唱した論文「[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)」は、2019年5月にはディープラーニングのカンファレンス ICLR 2019で [Best Paper Award](https://iclr.cc/Conferences/2019/Awards) に選ばれた。

ニューラルネットワークの設定（ハイパーパラメータ）を変えることによる出力結果の変化を直感的に確かめることができるサイト。

![スクリーンショット 2020-06-27 21 45 25](https://user-images.githubusercontent.com/5207601/85922615-bbf6cf00-b8bf-11ea-80a5-91a844d82d54.png)
[A Neural Network Playground](https://playground.tensorflow.org/)

##### [誤差逆伝播法（バックプロパゲーション）](https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%83%97%E3%83%AD%E3%83%91%E3%82%B2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3)
ニューラルネットワークの出力と正答データとの差（誤差）が後ろ（逆）のノードへと伝播するように計算を行い、その重みを調整する手法。
損失関数が小さくなるように各ノードの重みを調整する。

##### 損失関数
出力と正答データの誤差の和

## 学習の流れ

1. 教師データを用いて予測計算をする。その際の予測値を正解ラベルと比較して誤差を計算する。
1. 予測値は左から右へと順番に伝わる（順伝播）
1. 上記をいくつかの教師データについて繰り返し、誤差を足し合わせる
1. 累計された誤差が小さくなるように、勾配降下法を用いて各枝の重みを更新する。
   枝の重みは右から左へと順番に更新される（誤差逆伝播法）
1. 上記を繰り返す

### 学習（イテレーション）の方法

##### バッチ学習
1回の学習ですべてのデータを読み込んで学習する。
メモリの大きさが増加するが、すべてのデータを均等に扱える。
また、計算時間は長くなるので、モデルの更新に時間がかかる。

##### ミニバッチ学習
1回の学習で「バッチサイズ」として設定した数のデータを読み込んで学習する。
学習結果は最後に読み込んだデータに引っ張られるため、学習順序によって性能が変わるケースがある。

##### オンライン学習
1回の学習で1つのデータを読み込んで学習する。
学習サイクルが速く、新しいデータが入るとすぐそのデータが学習されたモデルが手に入る。

ミニバッチ学習と同様に、学習結果は最後に読み込んだデータに引っ張られるため、学習順序によって性能が変わるケースがある。

## 従来の機機械学習との違い

機械学習では、対象領域の知識に基づいて適切に手法を選択する必要があるが、ディーブラーニングでは、表現力の高い「深い関数」を用いるため、データと計算量されあれば精度が上がる。

ディーブラーニングに適した学習データは、正規化されているものよりも、画像や音声そのもの、膨大なテキストなど。
どうやてデータを集めるか、どうやってアノテーション（ラベルやメダデータを与えて学習データを整備）するかが課題となる。

## ディープラーニングのアプローチ

##### オートエンコーダ
2006年に発表され、今日のディーブラーニング隆盛のきっかけともなった技術。

次元数を減らしたニューロン層を重ねていくことで特徴の圧縮を行うエンコーダと逆の構造を持つデコーダーを接続したもの。
砂時計型の構造をもつニューラルネットワークで、入力側から半分をエンコーダ、残り半分をデコーダーと呼ぶ。

入力層と出力層のノード数が等しく、中間層のノード数が入出力層よりが少ない。
入力情報と同じ出力情報を再現するこを目指す（「正解ラベル」として「入力自身」を用いる）。

##### 次元削減
オートエンコーダにおいて、隠れ層で特徴的な情報だけに次元を圧縮すること。

##### ディープオートエンコーダ
オートエンコーダを順番に学習させ、それを積み重ねる手法。

ディープニューラルネットワークのように一気にすべての層を学習するのではなく、**入力層に近い層から順番に学習されるという、逐次的な方法をとった。**
事前学習とファインチューニングの工程で構成される。

##### 事前学習
オートエンコーダを順番に学習していく手法。

##### ファインチューニング
最後にロジスティック回帰層（シグモイド関数あるいはソフトマックス関数による出力層）を足すことで、教師学習を実現する。

これにより、ネットワーク全体は隠れ層が複数あるディープニューラルネットワークになる。

事前学習を終え、ロジスティック回帰層を足したら、最後の仕上げとしてディープニューラルネットワーク全体で学習を行い重みを調整する。

## [活性化関数](https://en.wikipedia.org/wiki/Activation_function)

入力信号の総和を出力信号に変換する関数。

出力層で使用する活性化関数は、回帰問題では恒等関数、分類問題ではソフトマックス関数を一般的に利用する。

パーセプトロンとニューラルネットワークの主な違いは、活性化関数。
パーセプトロンではステップ関数、ニューラルネットワークではシグモイド関数など非線形関数を用いる。

##### ステップ関数
![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Activation_binary_step.svg/800px-Activation_binary_step.svg.png)

微分できないのでニューラルネットワークの学習で実際に使われることはない。

##### [シグモイド関数](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Swish.svg/800px-Swish.svg.png)

入力を 0 ~ 1の間に値に変換する性質を持つ関数。
ステップ関数と形が似ていて、なめらかなので微分できる関数として考案された。

最大勾配が 0.25 で勾配消失が起こりやすいため、通常は ReLU 関数などを使うのが主流。

##### [tanh (ハイパボリックタンジェント関数)](https://ja.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E7%B7%9A%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/800px-Activation_tanh.svg.png)

##### [ReLU関数](https://ja.wikipedia.org/wiki/%E6%AD%A3%E8%A6%8F%E5%8C%96%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/800px-Activation_rectified_linear.svg.png)

正則化機能を持たいない活性化関数で、勾配消失問題が起きにくく、最近の主流。
入力が 0 を超えていればその入力をそのまま出力し、0 以下ならば 0 を出力する。

##### ソフトマックス関数
出力を正規化して、確率として解釈する際に用いされる活性化関数。
分類問題の出力層付近で用いられることが一般的。

##### Leaky ReLU関数
![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Activation_prelu.svg/800px-Activation_prelu.svg.png)

##### Parametric ReLU

##### Randomized ReLU

## 学習率の最適化

ニューラルネットワークの最適化は、損失関数の値が小さくなるような重みを探し出すこと。

モデルの最適化によく利用されるのが「勾配降下法」で、SGD など様々な最適化アルゴリズムがある。
学習する際には「誤差逆伝播法」を用いるが、その際には「勾配消失問題」に注意する。

##### 学習率
どのくらいの幅でパラメータを修正するかを決めるハイパーパラメータ。

学習率が大きい値だと速く収束するかもしれないが、谷を行き過ぎて最適な解に収束しない場合もある。
学習率が小さい場合は、収束するまでに必要な繰り返し回数が増えるため、学習時間が長くなる。

ニューラルネットワークでは、学習率を動的に変化させるさまざまな工夫が提案されている。

##### [勾配降下法](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)

![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Non-Convex_Objective_Function.gif)

https://en.wikipedia.org/wiki/File:Non-Convex_Objective_Function.gif

重みを少しずつ更新して勾配が最小になる点を探索するアルゴリズム。

ディープニューラルネットワークの学習では、「局所最適解が求められればそれなりに誤差の値は小さくなるだろうから、それで妥協しよう」というスタンスに立つ。

- 局所最適解  
  園周辺では誤差の値は小さいが、最小値を実現するわけではない解

- 大局的最適解

  誤差の値を最も小さくする解

- 停留点  

  局所最適解でも大局的最適解でもないが、勾配が 0 になる点

- 鞍点（あんてん）  

  停留点のうち、ある方向から見ると極小値だが、別の方向から見ると極大値になる点

##### [誤差関数](https://ja.wikipedia.org/wiki/%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)

##### エポック
1エポックとは、学習において訓練データをすべて使い切ったときの回数に対応する。
（訓練データを何度学習に用いたか）

##### イテレーション
重みを何度更新したか

##### ミニバッチ勾配降下法
ミニバッチに含まれるすべてのデータについて誤差の総和を計算し、その総和を小さくするように重みを1回更新する。

ミニバッチは、いくつかの訓練データからランダムにサンプリングした小さなデータの集まり。

##### 勾配降下法
訓練データすべての誤差を計算し、重みを1回更新する（イテレーションとエポックが等しい）。

##### 蒸留
大きいネットワークの入出力を小さいネットワークに再学習される手法。

##### Adadelta

##### 勾配消失問題
誤差の勾配を逆伝播する過程において、勾配の値が消出し入力層付近での学習が進まなくなるディープニューラルネットワーク特有の現象。層が深いほど起こりやすい。

活性化関数が何度も作用することで、勾配が小さくなりすぎてしまうことが原因とされる。

## 最適化アルゴリズム

##### SGD (確率的勾配降下法)
無作為に選びだしたデータ（ミニバッチ）に対して行う勾配降下法。

訓練データ1つに対して、重みを1回更新する（データ１つごとにイテレーションが増える）
関数の形状が等方向的でないと非効率な経路で探索することが欠点。

##### Momentum
ボールが地面を転がるように、損失関数上での今までの動きを考慮することで SGD の振動を抑える手法。

##### AdaGrad
勾配降下法の学習率に関する手法。
パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法。

##### RMSProp
AdaGrad は学習雨を進めれば進めるほど、更新度合いは小さくなり、無限に学習すると、更新量は 0 になる。この問題を改善した手法が、RMSPop。

過去のすべての勾配を均一に加算していくのではなく、過去の勾配を徐々に忘れて新しい勾配の情報が大きく反映されるように加算する手法。

##### Adam
2015年に提案された、直感的には、Momentum と　AdaGrad を融合した手法。

先の2つの手法の利点を組み合わせることで、効率的にパラメータ空間を探索することが期待できる。
ハイパーパラメータの補正が行われていることも特徴。

## テクニック

##### ドロップアウト
重み更新の際（エポックごと）に一定の割合でランダムに枝を無効化する手法。単純だが効果が高い。
アンサンブル学習と近い関係にある。

##### early stopping
訓練データにオーバーフィッティングする前に学習を早めに打ち切る手法。

誤差関数が「予測値と正解tの誤差」であり、訓練データをもちいて最小化するアプローチしか取れない以上、同区数しても（訓練データ）にオーバーフィッティングしてしまうのは避けられない。

##### 重みの初期値の工夫
ディープニューラルネットワークでは活性化関数がかかっているので、正規化したデータが層を伝播するにつれて分布が徐々に崩れてしまう。

そのため乱数にネットワークの大きさに合わせた係数をかけることで、データの分布が崩れにくい初期値がいくつか考えられた。

シグモイド関数に対しては Xavier の初期値、ReLU関数に対しては He の初期値が良いとされている。

##### バッチ正規化
各層において活性化関数をかける前に伝搬してきたデータを正規化する処理を加える。
無理やりデータを変形しているので、それをどのように調整すればいいのかをネットワークが学習する。

学習がうまく行きやすくなるという利点以外にも、オーバーフィッティング（過学習）しにくくなることも知られている。 

## CNN: 畳み込みニューラルネットワーク

![](https://cdn-images-1.medium.com/freeze/max/1000/1*UgbuN6hIr34GOo_5Gzb-DA.png?q=20
)
https://mc.ai/deep-learning%E2%80%8A-%E2%80%8Aconvolutional-neural-networks-cnn/

特に画像認識に応答するために改良されたディープニューラルネットワーク。
自動運転技術など、非常に広く応用されている。

従来のシグモイド関数のような活性化関数では、層が深くなるにつれ勾配が小さくなってくのでうまく学習ができないので、勾配消失問題を解決した ReLU関数を用いる。

### 処理の流れ

1. 畳み込み層  

  フィルタを用いて積和演算 + 活性化関数の作用を行う層。
  元の画像の特徴が抽出された小さな画像である「特徴マップ」に変換される。

2. プーリング層  

  畳み込み層から「特徴マップ」を受け取り、平均値、最大値を用いてサブサンプリングを行う層。
  平均プーリング、最大プーリングによって更に小さな画像に変換される。

3. 全結合層  

  プーリング層から出力された画像データを、縦横に並んだ2次元データから、一列に並べたフラットな1次元データに変換する出力層。

### 手法

##### LeNet
1998年にヤン・ルカンらによって提案された、手書き数字認識を行うネットワーク。

シグモイド関数を使用して、サンプリングによって中間データのサイズ縮小を行っている。
初めての CNN。

### 畳み込み層

畳み込みフィルタと呼ばれる、画像中の特定の形状に反応するフィルタを画像に掛けけあわせる処理が行われる。
これらのフィルタは、学習によってラベルの判別に有効な形状になっている。

画像内に畳み込みフィルタを一致するような部分があれば、その部分が強調されて移されて、「特徴マップ」と呼ばれる画像を生成する。

### プーリング層

あるサイズのウィンドウを画像のすべての部分にあてはめ、そのウィンドウの中から１つだけ値を抽出して新しい画像に写す操舵をする。

##### マックスプーリング（Max Pooling）
ウィンドウの中でもっとも大きい値を抽出する。配列が小さくなるので、データ量を削減できる。

多くの CNN モデルで利用されている。

##### avgプーリング
ウィンドウの中の値の平均値を抽出する。

### 全結合層
畳み込み層、プーリング層の処理で得られた特徴マップを読み込み、これらに含まれる特徴量を抽出する。
最終的には、出力層に予測や分類の結果を出力する。

畳み込み処理と同様に、何層にも積み重ねることでより複雑かつ有効な特徴量を利用した処理が可能。

通常のニューラルネットワークと同じ構造を取る。

##### データ拡張
様々なパターンを網羅したデータを収集する代わりに、手元のデータから擬似的に別のデータを生成する手法。データの水増しとも言われる。

手元にあるデータそれぞれに対して、ランダムにいくつかの処理を施して新しいデータを作り出す。

- 上下左右にずらす
- 上下左右を反転する
- 拡大・縮小する
- 回転する
- 斜めに歪める
- 一部を切り取る
- コントラストを変える

### CNN の発展型

##### AlexNet
2012年の ILSVRC でトロント大学のジェフリー・ヒントン率いるチームが用いた手法。 

- 活性化関数に ReLU を使用
- LRN (Local Response Noralization) という、局所的正規化を行う層を用いる
- ドロップアウトを使用する

##### VGG
畳み込み層とプーリング層から構成される基本的な CNN。

重みのある層を全部で16層まで重ねてディープにしている。
3x3 の小さなフィルターによる畳み込み層を連続して行っている点が特徴。

##### GoogLeNet
基本的には CNN と同じ構成で、ネットワークが縦方向の深さだけでなく、横方向にも深さ（広がり）を持っているのが特徴。
横方向の幅は「インセプション構造」と呼ばえる。

##### Skip connection
層を飛び越えた結合

## RNN: リカレントニューラルネットワーク

![](https://miro.medium.com/max/1400/1*aIT6tmnk3qHpStkOX3gGcQ.png)
https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce

時系列データにおいて、ある時間的に近接した要素同士は影響を与え合う可能性が高いが、時間的に遠く離れた要素が影響を与えることは少ない。
この性質を使えば、パラメータの数を減らすことができる。これが RNN である。

時系列データを入力して、データから時間依存性を学習できるモデル。
内部に閉路（行って戻ってくる経路）を持つニューラルネットワーク。
RNN は過去の情報を保持できるため、過去の入力を参考に「時系列データの次の時点での値」を予測する。
自然言語処理への応用が盛んで、機械翻訳技術などに応用されている。

##### 入力重み衝突

##### 出力重み衝突

##### BPTT (Backpropagation Through Time)
RNN を順伝播型ネットワークに置き換えて誤差逆伝播法を適用する手法。

##### CTC
入出力間で系列長が違う場合のニューラルネットワークを用いた分類法。

##### LSTM (Long Short-Term Memory)
遠い過去の入力を現在の出力に反映する手法。

##### seq2seq
時系列データを入力・処理し、時系列データを出力するモデル。

代表的な応用として入力と出力を異なる言語列とする翻訳が実現できる。
これは NMT（ニューラル機械翻訳）と呼ばれ、従来の SMT（統計的機械翻訳）よりも大幅に性能が向上している。

##### GRU (Gated Recurrent Unit)

##### Bidirectional RNN
LSTM を2つ組み合わせることで、未来から過去方向も含めて学習できるモデル。

##### Sequence-to-Sequence
入力が時系列なら出力も時系列で予測する。自然言語処理を中心に研究されている分野。

##### RNN Encoder–Decoder

##### Attention
過去の時点それぞれの重みを学習することで、時間の重みをネットワークに組み込む。

## 転移学習

学習済みのネットワークを利用して新しいタスクの識別に利用する手法。

### 学習の方法

##### 特徴抽出
ディープニューラルネットワークの最後の層では、分類に有効な特徴量が伝わっていると考えられるため、最後の層を切り取って、代わりに SVM などの従来の機械学習分類器をつなげても、うまく分類できる。

##### ファインチューニング
学習済みのモデルを使い、最後の方の層だけをタスクに合わせて学習し直す（微調整する）手法。

最初の方の層で捉えられる特徴はタスクによって変わることはないが、最後の層で捉えられる特徴はタスクによって変わるため、学習をやり直す必要がある。

### 課題と関連分野

##### ドメイン適合
ドメインを変えて転移学習を行う手法。
（ex. 新聞記事を分析するように学習したモデルを初期値にして、ツイートを分析するモデルを作成する）

##### ドメイン混同
通常出力する値の他に、入力したデータのドメインも出力することで、モデルが特定の領域に特化しないようにする手法。

##### 負の転移
転移元と転移先のタスクがそれほど似ていないので、通常の学習よりも性能の悪いモデルが作られてしまうこと。

##### マルチタスク学習
複数のタスクについて同時に学習させること。通常の転移学習は一度に特定のタスクについてモデルを学習させる。

##### One-shot 学習
あるラベルについての訓練データが一つ（あるいは少数）しかなくても正しい出力ができる手法。
「一を聞いて十を知る」学習方法。

特定のラベルがついた訓練データが存在しない場合にそのラベルを出力するような、「Zero-shot 学習」と呼ばれる手法も研究されている。

## 深層強化学習

強化学習は、システムが「状態」を定義し、試行錯誤を重ねながら、自動的に報酬を最大化する「方策」を見つけることができる。
しかし、こうした問題はかなり厄介で、1990年代には活発に研究が行われたものの、2000年代に入るとその勢いも衰えてしまいました。

ディープラーニングの登場でその状況が打開されつつあり、さらに探索による先読みを組み合わせることで、AlphaGo（碁）の躍進につながった。

##### 強化学習
行動を学習する仕組み。

ある環境で、目的とする報酬（スコア）を最大化するためにはどのような行動をとっていけばいいかを学習していく。
機械学習では、想定する「状態」の数が非常に多くなったり、人による「状態」の想定に限界があったりした。

強化学習のアルゴリズムは、「モデルベース」と「モデルフリー」に大別される。

##### Q学習
価値ベースの方法で、「ある状態においてある行動をとることがどれくらい良いか」という行動の価値を行動していく。


### 改善モデル

強化学習の改善手法としては、以下の3種類に基づいている。

##### 方策ベース

##### 状態価値関数（Q関数）ベース

##### モデルベース

##### RAINBOWモデル
上記3つのモデルを入れた手法。

##### [BRETT](https://engineering.berkeley.edu/brett/)
カリフォルニア大学バークレー校が開発している、ディーブラーニングと強化学習を組み合わせたロボット。

報酬の設定を変更すれば、異なる動作を学習することができる。
一旦学習した内容はコピーして、同じタイプのロボットであれば同じように動かすことができる。

##### [AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo)
DeepMind社が開発した碁のプログラム。  
どのような手を打つべきかの探索には[モンテカルロ木探索](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%9C%A8%E6%8E%A2%E7%B4%A2)が用いられている。

##### [AlphaGo Zero](https://ja.wikipedia.org/wiki/AlphaGo_Zero)
2017年10月に発表された、棋譜を全く必要としない、完全に自己対局のみで学習していく碁のプログラム。
従来の AlphaGo を超える強さとなった。

##### [DQN (Deep Q-Network)](https://ja.wikipedia.org/wiki/DQN_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF))
DeepMind が考案した、Q学習の行動価値関数を、深い構造を持ったニューラルネットワークで置き換えたモデル。

アタリ社のゲームを学習された例では、49ゲームのうち29ゲームで、人間並、あるいはそれ以上のスコアを出した。

### [OpenAI Gym](https://gym.openai.com/)

強化学習のシュミレーション用プラットフォーム。

公式ドキュメントでさまざまなチュートリアルが用意されているので、実際に動かして強化学習の面白さを体験できる。

[![Image from Gyazo](https://i.gyazo.com/80b3e3b6c140c6a5767a8a07904a450f.gif)](https://gyazo.com/80b3e3b6c140c6a5767a8a07904a450f)
Link: [OpenAI Gym](https://gym.openai.com/)

## 深層生成モデル

ディーブラーニング技術は、識別や回帰だけでなく、これまで存在していなかったデータ、例えば架空の画像の生成にも利用できる。

一般に「生成モデル」と呼ばれるモデルを使うことで、AI 技術による「創作」が可能となる。
ディープニューラルネットワークを使った生成モデルは「深層生成モデル」と呼ばれる。

##### WaveNet

##### 生成モデル

##### [VAE (変分オートエンコーダ)](https://ja.wikipedia.org/wiki/%E5%A4%89%E5%88%86%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80%E3%83%BC)
オートエンコーダの中間層の圧縮された特徴表現の数値を確率分布に従うように変更し、さらにエンコーダの出力とデコーダーの入力をこれにあわせて変更したのもの。

確率分布に従うことで、圧縮された特徴表現の数値を変動させ、デコーダーの所期の性能（いろいろな猫の画像の生成）を達成できる。

##### [GAN (敵対的生成ネットワーク)](https://ja.wikipedia.org/wiki/%E6%95%B5%E5%AF%BE%E7%9A%84%E7%94%9F%E6%88%90%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)
2014年に[イアン・グッドフェロー](https://ja.wikipedia.org/wiki/%E3%82%A4%E3%82%A2%E3%83%B3%E3%83%BB%E3%82%B0%E3%83%83%E3%83%89%E3%83%95%E3%82%A7%E3%83%AD%E3%83%BC)らによって発表された、生成ネットワークと識別ネットワークからなる教師なし学習法。

トレーニングに利用したデータに類似した出力を生成できるため、希少データの水増しや、作風をまねた絵画や楽曲の生成など、様々な応用が進められている。

- 生成ネットワーク  

  ランダムノイズベクトルから入力データのレプリカを作ろうとする
  訓練データと同じようなデータを生成する

- 識別ネットワーク  

  そのデータが訓練データから来たものか、生成ネットワークから来たものかを識別する

##### DCGAN (Deep Convolutional GAN)

## GNN: グラフニューラルネットワーク

グラフ構造（データ間のつながりを持った構造）のデータを入力とするニューラルネットワーク。

GNN やオートエンコーダ、RNN を構築することができるため、広範囲の応用が可能。
分子構造の予測や自然言語処理などへの応用がある。

## 用語

##### [ノーフリーランチ定理](ノーフリーランチ定理)
「あらゆる問題に対して万能なアルゴリズムは存在しない」という定理。

##### 単純パーセプトロン
1958年にアメリカの心理学者[フランク・ローゼンブラット](https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%A9%E3%83%B3%E3%82%AF%E3%83%BB%E3%83%AD%E3%83%BC%E3%82%BC%E3%83%B3%E3%83%96%E3%83%A9%E3%83%83%E3%83%88)が提案したニューラルネットワークの元祖のモデル。

##### 多層パーセプトロン

## データセット

こちらのドキュメントに機械学習で利用可能なデータセットがまとまっている。

https://github.com/arXivTimes/arXivTimes/tree/master/datasets

##### MNIST（エムニスト）
手書き文字認識学習用データ。

手書き数字画像60,000枚と、テスト画像10,000枚を集めた、画像データセット。
さらに、手書きの数字「0〜9」に正解ラベルが与えられるデータセットでもあり、画像分類問題で人気の高いデータセット。

URL: http://yann.lecun.com/exdb/mnist/

##### Imagenet
スタンフォード大学がインターネット上から画像を集め分類したデータセット。

一般画像認識用に用いられる。ImageNetを利用して画像検出・識別精度を競うThe ImageNet Large Scale Visual Recognition Challenge（ILSVRC）などコンテストも開かれている。

URL: http://www.image-net.org/
# 🔬 ディープラーニングの研究分野

## 画像認識分野

##### [AlexNet (アレックスネット)](https://en.wikipedia.org/wiki/AlexNet)
2012年、ILSVRC で従来の SVM に替わりディープラーニングに基づくモデルで初めて優勝した。

筆頭開発者であるアレックス・クリジェフスキーの名前から、「アレックスネット」と呼ばれている。

### R-CNN

![](https://media.geeksforgeeks.org/wp-content/uploads/20200219161502/RCNN1.png)
https://www.geeksforgeeks.org/r-cnn-vs-fast-r-cnn-vs-faster-r-cnn-ml/

##### R-CNN（Region Convolutional Neural Network）
空間認識にすぐれている CNN を分割された領域ごとに適用するディープニューラルネットワーク。

関心領域（ROI）の切り出しには、HOG など CNN ではない従来の手法を用いる。ROI の画像切り出しの後に領域ごとに個別に CNN を呼びだす二段階のモデルのため、時間がかかっていた。

画像上の矩形領域（長方形）、バウンディングボックスで、領域を切り出す。

##### fast RCNN
領域の切り出しと切り出した領域の物体認識を同時に行うモデル。

##### faster RCNN
fast RCNN を更に改良したモデル。
ほぼ実時間（1秒あたり16フレーム）で入力画像から関心領域の切り出しと認識がきるようになった。

##### YOLO (You Only Look Once)
領域の切り出しと認識を同時に行う CNN。

##### SSD (Single Shot Detector)
領域の切り出しと認識を同時に行う CNN。

### セマンティックセグメンテーション・インスタンスセグメンテーション

##### セマンティックセグメンテーション
R-CNN のような矩形領域を切り出すのではなく、より詳細な領域分割を得るモデル。
各画素がどのカテゴリーに属するかを求める手法。

同じカテゴリーに属する複数の物体が同一ラベルとして扱われる。

##### インスタンスセグメンテーション
個々の物体ごとにカテゴリーを認識させる。

##### FCN (完全畳み込みネットワーク)
セマンティックセグメンテーションを実現するネットワークモデル。

FCN とは文字通りすべての層が畳み込み層であるモデル。

入力画像の画素数だけ出力層が必要、つまり出力層には、縦画素数 x 横画素数 x カテゴリー数の出力ニューロンが用意される。

##### アンサンプリング
最終出力層で入力層と同じ解像度を得るために、下位層のプーリング層の情報を用いて詳細な解像度を得る手法。

CNN では畳み込み演算によって畳み込みのカーネル幅（受容野）だけ近傍の入力刺激を加えて計算することになるため、上位層では下位層にくらべて受容野が大きくなり、その影響で画像サイズは小さくなる。

##### Mask RCNN

## 自然言語処理

### 単語の意味を表すベクトル空間モデル

##### [word2vec](https://ja.wikipedia.org/wiki/Word2vec)
文章中の単語は、記号の集まりとして表現できる。この記号をベクトルとして表現することで、ベクトル間の距離や関係として単語の意味を表現するモデル。

Googleのトマス・ミコロフ率いる研究者チームによって2013年に作成された。

単語の意味をベクトル空間の中に表現したと考えられるため、「単語埋め込みモデル（word embedding models）」とも呼ばれる。

word2vec には以下の2つの手法がある。

##### スキップグラム（Skip-gram）
ある単語を与えて周辺の単語を予測するモデル。

##### CBOW
周辺の単語を与えてある単語を予測するモデル。

### 文章の意味表現

##### fastText
2013年に word2vec を提案したトマス・ミコロフらによって開発されたモデル。

word2vec との変更点は、単語の表現に文字情報も含めること。文字データを援用することで訓練データには存在しない単語（Out Of Vocabulary: OOV）を表現することを可能にした。

##### ELMo
[アレンインスティチュート](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%AC%E3%83%B3%E8%84%B3%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80) によって開発された文章表現を得るモデル。

2層の双方向リカレントネットワーク言語モデルの内部状態から計算される。fastText と同じく OOV であっても意味表現を得ることが可能。

##### 普遍埋め込みモデル
1対多のマルチタスク学習により、複数課題間に共通の普遍的な文章埋め込み表現を学習するモデル。

### その他の応用

##### NIC (ニューラル画像脚注付け)
画像認識をする CNN と言語モデルとしてのリカレントニューラルネットワークを組み合わせて、画像に脚注をつける手法。

##### NTM (ニューラルチューリングマシン)
チーリングマシンをニューラルネットワークで実現する試み。

## 音声認識

##### WaveNet
音声合成と音声認識の両者を行うことができるモデル。

## 強化学習

##### DQN (Deep Q-Network)
DeepMind が考案した、Q学習の行動価値関数を、深い構造を持ったニューラルネットワークで置き換えたモデル。

##### MCTS (モンテカルロ木探索)
モンテカルロ法を使った木の探索。

ヒューリスティクス（途中で不要な探索をやめ、ある程度の高確率で良い手を導ける）な探索アルゴリズムである。

##### AlphaGo Zero
2017年10月に発表された、棋譜を全く必要としない、完全に自己対局（セルフプレイ）のみで学習していく碁のプログラム。
従来の AlphaGo を超える強さとなった。

# 🏛️ 法律・倫理・現行の議論

##### プライバシーバイ・デザイン

##### 倫理的に調和された設計
2016年にIEEEが出したレポート

##### データの利用条件
  1. 著作権法
  2. 不正競争防止法
  3. 個人情報保護法
  4. 個別の契約
  5. その他の理由により、データの利用に制約がかかっている場合

##### 日本の著作権法
著作権法47条の７によって、著作者にモダンで記録や翻案をしても適法となっている。2018年の著作権法改正によって、学習データを第三者と共有したり、一般に販売したり、ネット上で公開するとことも、一定の条件下で適法となっている（改正著作権法30条4）

##### オープン・イノベーション

##### [AI・データの利用に関する契約ガイドライン](https://www.meti.go.jp/press/2018/06/20180615001/20180615001.html)

##### データセットの偏り

##### プライバシーリスクを低減するデータ加工

##### 協調フィルタリング

##### [FAT (Fairness, Accountability, and Transparency)](https://www.fatml.org/)
AIの公平性、説明可能性、透明性

##### 敵対的な攻撃

##### 知的財産法

##### GDPR
欧州経済領域（EEA）内の個人データの保護を規定する法律であり、データ管理者及びデータ処理者に対し、個人データの取り扱いや移転に係る義務を定めている。
2016年4月に制定され、2018年5月に施行された。

##### アルゴリズムバイアス
Google Photos がアフリカ系の女性に「ゴリラ」とラベル付をしてしまった事件

##### インセンティブ設計

##### クライシスマネージメント  
危機管理対応

##### エスカレーション

##### 透明性レポート
  - [Twitter](https://transparency.twitter.com/ja.html)
  - [Google](https://transparencyreport.google.com/?hl=ja)
  - [Facebook](https://transparency.facebook.com/)

##### [PAI(Partnership on AI)](https://www.partnershiponai.org/)

##### [アシロマAI原則](https://futureoflife.org/ai-principles-japanese/)
人類の存続の危機を回避することを目的とする組織、[Future of Life Institute（FLI）](https://futureoflife.org/) が、2017年2月にアメリカのアシロマで発表した、人工知能（AI）の研究課題、倫理と価値、長期的な課題を23にまとめたガイドライン。

##### [AIネットワーク社会推進会議](https://www.soumu.go.jp/main_sosiki/kenkyu/ai_network/index.html)

##### [人間中心のAI社会原則検討会議](https://www8.cao.go.jp/cstp/tyousakai/humanai/index.html)
2018年5月に内閣府のCSTIの下に設置された会議。
AIに関する倫理や中長期的な研究開発・利活用などについて、産学民官のマルチステークホルダーによる幅広い視野からの調査・検討が行われ、2019年3月に「人間中心のAI社会原則」が決定された。

##### XAI (説明可能なAI)
ディーブラーニングを使う機械学習では、特徴抽出も自動化されているため、与えられたデータに対する推論過程そのものが「見えない」状態となる。
しかし、自動運転や医療診断など、推論結果が重大な問題を引き起こす可能性があるものに関しては、原因究明が要請される。

# 🏢 事例・サービス
## 社会実装
- [BRETT: Deep-learning robot](https://engineering.berkeley.edu/brett/)
- [AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo)
- [Google DeepMind's Deep Q-learning playing Atari](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning)
- [分身ロボットカフェ DAWN](https://dawn2019.orylab.com/)

## Cloud

### Google
- [Natural Language](https://cloud.google.com/natural-language?hl=ja)

### AWS

### Microsoft

## Community
- [Kaggle](https://www.kaggle.com/)

## Library

##### [scikit-learn](https://scikit-learn.org/stable/index.html)
Python でもっとも利用されている機械学習ライブラリ。
教師あり学習、教師なし学習、次元削減のさまざまなアルゴリズムを備えている。

##### [NumPy](https://numpy.org/)
多次元配列を扱うためのさまざまな機能がパッケージされたライブラリ。

##### [pandas](https://pandas.pydata.org/)
配列操作の中でもデータ分析に特化したライブラリ。
表形式のデータに対するさまざまな処理をサポートしている。

### 文章データの前処理

##### mecab
日本語の形態素解析ライブラリ。

##### [NLTK](https://www.nltk.org/)
英語の形態素解析ライブラリ。

### 画像データの前処理

##### [OpenCV](https://opencv.org/)
コンピュータで画像や動画を処理するためのさまざまな処理を実装できるライブラリ。

画像のぼかしや二値化、グレースケールに拡大縮小、回転などの基本操作から、エッジ検出やヒストグラムの計算など、機械学習アルゴリズムに入力するために必要な前処理を網羅している。

### データの可視化

##### [matplotlib](https://matplotlib.org/)
折れ線グラフや棒グラフなどの基本的なグラフから、ヒストグラムなどの統計用グラフや3D散布図など、多様な表示形式を利用できる。

##### [seaborn](https://seaborn.pydata.org/)

## Framework

##### [TensorFlow](https://www.tensorflow.org/?hl=ja)
Google の開発するディープラーニングのフレームワーク。

TensorBoard という可視化ソフトフェアが付属しており、計算グラフを表示できるほか、学習がどのように進んでいったかわかりやすく可視化できる。

##### PyTorch
Facebook が開発している TensorFlow と双璧をなすフレームワーク。

デフォルトで Define-by-Run アプローチが取られ、動的に計算グラフを生成していくため、柔軟な計算ができる。

##### Keras
非常に簡単なコードでモデルを組み込み学習を行うことができる。
学習自体は裏で TensorFlow が行っている。
Keras は TensorFlow のラッパー。

##### [Caffe](https://caffe.berkeleyvision.org/)

## 計算
Docker: https://hub.docker.com/r/continuumio/anaconda3

```sh
# Download Docker image
docker pull continuumio/anaconda3

# Start a Jupyter Notebook server and interact with Anaconda via your browser
docker run -i -t -p 8888:8888 continuumio/anaconda3 /bin/bash -c "/opt/conda/bin/conda install jupyter -y --quiet && mkdir /opt/notebooks && /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser --allow-root"
```

# 📚 参考資料

## 書籍
### 入門
- [機械学習入門 ボルツマン機械学習から深層学習まで](https://www.ohmsha.co.jp/book/9784274219986/)
- [なっとく！ディープラーニング](https://www.shoeisha.co.jp/book/detail/9784798166247)

### G検定
- [深層学習教科書 ディープラーニング G検定公式テキスト](https://www.shoeisha.co.jp/book/detail/9784798157559)
- [徹底攻略 ディープラーニングG検定 ジェネラリスト 問題集](https://book.impress.co.jp/books/1118101076)
- [AI白書 2020](https://www.ipa.go.jp/ikc/publish/ai_hakusyo.html)

### アルゴリズム
- [ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装](https://www.oreilly.co.jp/books/9784873117584/)
- [仕事ではじめる機械学習](https://www.oreilly.co.jp/books/9784873118215/)
- [図解即戦力 機械学習・ディープラーニングのしくみと技術がこれ1冊でしっかりわかる教科書](https://gihyo.jp/book/2019/978-4-297-10640-9)


## WEB
- [ニコニコAIスクール](http://nico2.ai/)
  - 最終回: 脳神経科学と汎用人工知能 (福島邦彦)


- [G検定模擬テスト - Study-A](http://study-ai.com/generalist/)

### Kaggle
- [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)
- [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)
- [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
- [Deep Learning](https://www.kaggle.com/learn/deep-learning)

### Aidemy
- [はじめてのAI](https://aidemy.net/courses/9200)
- [機械学習概論](https://aidemy.net/courses/2010)
- [ディープラーニング基礎](https://aidemy.net/courses/5090)

### Google
- [How Google does Machine Learning](https://ja.coursera.org/learn/google-machine-learning-jp)
- [Machine Learning Crash Course with TensorFlow APIs](https://developers.google.com/machine-learning/crash-course)

### Rebuild
- [223: Ear Bleeding Pods (higepon)](https://rebuild.fm/223/#t=00:00)
- [254: Udon Ecosystem (higepon)](https://rebuild.fm/254/#t=36:24)

## Youtube

### 松尾豊
- [2014/11/01 人工知能が閻魔大王になる日 - Video News](https://www.youtube.com/watch?v=U-O4eINZNXE)
- [2015/06/05 人工知能は人間を超えるか ディープラーニングの先にあるもの - #LYVE](https://www.youtube.com/watch?v=lqywEafvq_Q)
- [2015/11/03 人工知能の未来～ディープラーニングの先にあるもの - GLOBIS](https://www.youtube.com/watch?v=GbmKWY7SLng)
- [2016/08/02 人工知能は人間を超えるか - SoftBank World 2016](https://www.youtube.com/watch?v=7bvfl_M5vPQ)

### NDIVIA
- [ディープ・ラーニング最新技術情報 何故GPUはディープラーニングに向いているか](https://www.youtube.com/watch?v=1aHQ2tVVlj8)

### Google
- [2017/06/29 Google のデータサイエンティストが語る現場で使える機械学習入門](https://www.youtube.com/watch?v=PARsDyRJMWE)

## Email Newsletter
- [Deep Learning Weekly](https://www.deeplearningweekly.com/)
- [PyCoder’s Weekly](https://pycoders.com/)
- [Kaggle Weekly](https://www.getrevue.co/profile/upura)
