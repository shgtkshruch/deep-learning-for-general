# 🦄 ディープラーニング

ディープニューラルネットワークを用いた機械学習の手法。
2010年代に脚光を浴びたが、アルゴリズム自体は1960年代にはすでに考案されていた。 

ニューラルネットワークを3層より多層にしても学習精度が上がらない壁にぶつかったが、自己符号化器の研究などを足場にして、層を深くしても学習することが可能となった。

人工知能の大家[マービン・ミンスキー](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%BC%E3%83%93%E3%83%B3%E3%83%BB%E3%83%9F%E3%83%B3%E3%82%B9%E3%82%AD%E3%83%BC)によって、特定の条件下の単純パーセプトロンでは直線で分離できるような単純な問題しか解けないということが指摘されたが、バックプロパゲーション（誤差逆伝播法）で克服できることが示された。

「深い関数を使った最小二乗法」

「シグモイド関数など活性化関数を使って非線形性を入れ、多層に構成した関数を使った、損失関数の最小化」

## 変遷
##### 1958年
パーセプトロン

##### 1969年
パーセプトロンの性能と限界に関する論文
（冬の時代が始まる）

##### 1986年
バックプロパゲーション（誤差逆伝播法）
（第二次 AI ブーム）

##### 2006年
自己符号化器
（第3次 AI プームのきかっけ）

##### 2012年
ILSVRC でトロント大学の SuperVision が優勝

##### 2017年
AlphaGo が人間のプロ囲碁棋士を破る

## ニューラルネットワーク

人間の神経回路（ニューロン）の構造をモデル化したネットワーク。

##### ニューロン
単純な数値予測ができ予測器。ニューラルネットワークの最小単位。
重みが乗算された入力を受け取り、それらを総和して出力する。

##### ニューラルネットワーク
ニューロンをたくさんつなげててきる予測器。入力が行わる層を「入力層」、出力される層を「出力層」、それ以外の層を「中間層」または「隠れ層」と呼ぶ。

また、入力層の各ノードが隠れ層のすべてのノードと結合しているような層を「全結合層」と呼ぶ。
人間の脳を模倣したモデルではなく、「人間の脳の仕組みの一部」を模倣している。

##### [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3)
1982年に福島邦彦氏によって発表された、畳み込みニューラルネットワークの発想の基となったネットワークモデル。

##### 宝くじ仮説
「ランダムに初期化された密なニューラルネットワークは、たまたまうまく学習ができるように初期化されたサブネットワークが含まれており、このサブネットは、学習が進むにつれて他のサブネットよりも優れているので早く学習が進み、他の劣るサブネットの活性が抑制される」という仮説。

この仮説を提唱した論文「[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)」は、2019年5月にはディープラーニングのカンファレンス ICLR 2019で [Best Paper Award](https://iclr.cc/Conferences/2019/Awards) に選ばれた。

ニューラルネットワークの設定（ハイパーパラメータ）を変えることによる出力結果の変化を直感的に確かめることができるサイト。

![スクリーンショット 2020-06-27 21 45 25](https://user-images.githubusercontent.com/5207601/85922615-bbf6cf00-b8bf-11ea-80a5-91a844d82d54.png)
[A Neural Network Playground](https://playground.tensorflow.org/)

##### [誤差逆伝播法（バックプロパゲーション）](https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%83%97%E3%83%AD%E3%83%91%E3%82%B2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3)
ニューラルネットワークの出力と正答データとの差（誤差）が後ろ（逆）のノードへと伝播するように計算を行い、その重みを調整する手法。
損失関数が小さくなるように各ノードの重みを調整する。

##### 損失関数
出力と正答データの誤差の和

## 学習の流れ

1. 教師データを用いて予測計算をする。その際の予測値を正解ラベルと比較して誤差を計算する。
1. 予測値は左から右へと順番に伝わる（順伝播）
1. 上記をいくつかの教師データについて繰り返し、誤差を足し合わせる
1. 累計された誤差が小さくなるように、勾配降下法を用いて各枝の重みを更新する。
   枝の重みは右から左へと順番に更新される（誤差逆伝播法）
1. 上記を繰り返す

### 学習（イテレーション）の方法

##### バッチ学習
1回の学習ですべてのデータを読み込んで学習する。
メモリの大きさが増加するが、すべてのデータを均等に扱える。
また、計算時間は長くなるので、モデルの更新に時間がかかる。

##### ミニバッチ学習
1回の学習で「バッチサイズ」として設定した数のデータを読み込んで学習する。
学習結果は最後に読み込んだデータに引っ張られるため、学習順序によって性能が変わるケースがある。

##### オンライン学習
1回の学習で1つのデータを読み込んで学習する。
学習サイクルが速く、新しいデータが入るとすぐそのデータが学習されたモデルが手に入る。

ミニバッチ学習と同様に、学習結果は最後に読み込んだデータに引っ張られるため、学習順序によって性能が変わるケースがある。

## 従来の機機械学習との違い

機械学習では、対象領域の知識に基づいて適切に手法を選択する必要があるが、ディーブラーニングでは、表現力の高い「深い関数」を用いるため、データと計算量されあれば精度が上がる。

ディーブラーニングに適した学習データは、正規化されているものよりも、画像や音声そのもの、膨大なテキストなど。
どうやてデータを集めるか、どうやってアノテーション（ラベルやメダデータを与えて学習データを整備）するかが課題となる。

## ディープラーニングのアプローチ

##### [オートエンコーダ](https://jp.mathworks.com/discovery/autoencoder.html)
2006年に発表され、今日のディーブラーニング隆盛のきっかけともなった技術。
3層ニューラルネットにおいて、入力層と出力層に同じデータを用いて**教師なし学習**させたもの。

次元数を減らしたニューロン層を重ねていくことで特徴の圧縮を行うエンコーダと逆の構造を持つデコーダーを接続したもの。
砂時計型の構造をもつニューラルネットワークで、入力側から半分をエンコーダ、残り半分をデコーダーと呼ぶ。

入力層と出力層のノード数が等しく、中間層のノード数が入出力層よりが少ない。
入力情報と同じ出力情報を再現するこを目指す（「正解ラベル」として「入力自身」を用いる）。

##### 次元削減
オートエンコーダにおいて、隠れ層で特徴的な情報だけに次元を圧縮すること。

##### ディープオートエンコーダ
オートエンコーダを順番に学習させ、それを積み重ねる手法。

ディープニューラルネットワークのように一気にすべての層を学習するのではなく、**入力層に近い層から順番に学習されるという、逐次的な方法をとった。**（層ごとの貪欲法）
事前学習とファインチューニングの工程で構成される。

##### 事前学習
オートエンコーダを順番に学習していく手法。

##### ファインチューニング
最後にロジスティック回帰層（シグモイド関数あるいはソフトマックス関数による出力層）を足すことで、教師学習を実現する。

これにより、ネットワーク全体は隠れ層が複数あるディープニューラルネットワークになる。

事前学習を終え、ロジスティック回帰層を足したら、最後の仕上げとしてディープニューラルネットワーク全体で学習を行い重みを調整する。

## [活性化関数](https://en.wikipedia.org/wiki/Activation_function)

入力信号の総和を出力信号に変換する関数。

出力層で使用する活性化関数は、回帰問題では恒等関数、分類問題ではソフトマックス関数を一般的に利用する。

パーセプトロンとニューラルネットワークの主な違いは、活性化関数。
パーセプトロンではステップ関数、ニューラルネットワークではシグモイド関数など非線形関数を用いる。

##### ステップ関数
![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Activation_binary_step.svg/800px-Activation_binary_step.svg.png)

微分できないのでニューラルネットワークの学習で実際に使われることはない。

##### [シグモイド関数](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Swish.svg/800px-Swish.svg.png)

入力を 0 ~ 1の間に値に変換する性質を持つ関数。
ステップ関数と形が似ていて、なめらかなので微分できる関数として考案された。

最大勾配が 0.25 で勾配消失が起こりやすいため、通常は ReLU 関数などを使うのが主流。

##### [tanh (ハイパボリックタンジェント関数)](https://ja.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E7%B7%9A%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/800px-Activation_tanh.svg.png)

##### [ReLU関数](https://ja.wikipedia.org/wiki/%E6%AD%A3%E8%A6%8F%E5%8C%96%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/800px-Activation_rectified_linear.svg.png)

正則化機能を持たいない活性化関数で、勾配消失問題が起きにくく、最近の主流。
入力が 0 を超えていればその入力をそのまま出力し、0 以下ならば 0 を出力する。

##### ソフトマックス関数
出力の総和を 1 に正規化して、確率として解釈する際に用いされる活性化関数。
分類問題の出力層付近で用いられることが一般的。

##### Leaky ReLU関数
![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Activation_prelu.svg/800px-Activation_prelu.svg.png)

##### Parametric ReLU

##### Randomized ReLU

## 学習率の最適化

ニューラルネットワークの最適化は、損失関数の値が小さくなるような重みを探し出すこと。

モデルの最適化によく利用されるのが「勾配降下法」で、SGD（確率的勾配降下法）など様々な最適化アルゴリズムがある。
学習する際には「誤差逆伝播法」を用いるが、その際には「勾配消失問題」に注意する。

##### 学習率
どのくらいの幅でパラメータを修正するかを決めるハイパーパラメータ。

学習率が大きい値だと速く収束するかもしれないが、谷を行き過ぎて最適な解に収束しない場合もある。
学習率が小さい場合は、収束するまでに必要な繰り返し回数が増えるため、学習時間が長くなる。

ニューラルネットワークでは、学習率を動的に変化させるさまざまな工夫が提案されている。

##### [勾配降下法](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)

![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Non-Convex_Objective_Function.gif)

https://en.wikipedia.org/wiki/File:Non-Convex_Objective_Function.gif

重みを少しずつ更新して勾配が最小になる点を探索するアルゴリズム。

ディープニューラルネットワークの学習では、「局所最適解が求められればそれなりに誤差の値は小さくなるだろうから、それで妥協しよう」というスタンスに立つ。

- 局所最適解  
  園周辺では誤差の値は小さいが、最小値を実現するわけではない解

- 大局的最適解

  誤差の値を最も小さくする解

- 停留点  

  局所最適解でも大局的最適解でもないが、勾配が 0 になる点

- 鞍点（あんてん）  

  停留点のうち、ある方向から見ると極小値だが、別の方向から見ると極大値になる点

##### [誤差関数](https://ja.wikipedia.org/wiki/%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)

##### エポック
1エポックとは、学習において訓練データをすべて使い切ったときの回数に対応する。
（訓練データを何度学習に用いたか）

##### イテレーション
重みを何度更新したか

##### ミニバッチ勾配降下法
ミニバッチに含まれるすべてのデータについて誤差の総和を計算し、その総和を小さくするように重みを1回更新する。

ミニバッチは、いくつかの訓練データからランダムにサンプリングした小さなデータの集まり。

##### 勾配降下法
訓練データすべての誤差を計算し、重みを1回更新する（イテレーションとエポックが等しい）。

##### 蒸留
大きいネットワークの入出力を小さいネットワークに再学習される手法。

##### 勾配消失問題
誤差の勾配を逆伝播する過程において、勾配の値が消出し入力層付近での学習が進まなくなるディープニューラルネットワーク特有の現象。層が深いほど起こりやすい。

活性化関数が何度も作用することで、勾配が小さくなりすぎてしまうことが原因とされる。

##### 最急降下法
1回のパラメータ更新でデータ全部を使っているため、一気にパラメータを更新できますが、計算量が大きくかつ最適解ではない極小値に陥ってしまった場合抜け出せない、という欠点がある。

## 最適化アルゴリズム

まとめ記事。
https://postd.cc/optimizing-gradient-descent/

##### SGD (確率的勾配降下法)
無作為に選びだしたデータ（ミニバッチ）に対して行う勾配降下法。

最急降下法の欠点を克服するために、ランダム性を含んだ最急降下法。

訓練データ1つに対して、重みを1回更新する（データ１つごとにイテレーションが増える）
関数の形状が等方向的でないと非効率な経路で探索することが欠点。

##### Momentum
ボールが地面を転がるように、損失関数上での今までの動きを考慮することで SGD の振動を抑える手法。

##### AdaGrad
勾配降下法の学習率に関する手法。
パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法。

##### Adadelta
Adagradの発展形で、急速かつ単調な学習率の低下を防ぐ手段を探る手法。

##### RMSProp
AdaGrad は学習を進めれば進めるほど、更新度合いは小さくなり、無限に学習すると、更新量は 0 になる。この問題を改善した手法が、RMSPop。



過去のすべての勾配を均一に加算していくのではなく、過去の勾配を徐々に忘れて新しい勾配の情報が大きく反映されるように加算する手法。

##### Adam
2015年に提案された、直感的には、Momentum と AdaGrad を融合した手法。

先の2つの手法の利点を組み合わせることで、効率的にパラメータ空間を探索することが期待できる。
ハイパーパラメータの補正が行われていることも特徴。

## テクニック

##### ドロップアウト
重み更新の際（エポックごと）に一定の割合でランダムに枝を無効化する手法。単純だが効果が高い。
アンサンブル学習と近い関係にある。

##### early stopping
訓練データにオーバーフィッティングする前に学習を早めに打ち切る手法。

誤差関数が「予測値と正解tの誤差」であり、訓練データをもちいて最小化するアプローチしか取れない以上、同区数しても（訓練データ）にオーバーフィッティングしてしまうのは避けられない。

##### 重みの初期値の工夫
ディープニューラルネットワークでは活性化関数がかかっているので、正規化したデータが層を伝播するにつれて分布が徐々に崩れてしまう。

そのため乱数にネットワークの大きさに合わせた係数をかけることで、データの分布が崩れにくい初期値がいくつか考えられた。

シグモイド関数に対しては Xavier の初期値、ReLU関数に対しては He の初期値が良いとされている。

##### バッチ正規化
各層において活性化関数をかける前に伝搬してきたデータを正規化する処理を加える。
無理やりデータを変形しているので、それをどのように調整すればいいのかをネットワークが学習する。

学習がうまく行きやすくなるという利点以外にも、オーバーフィッティング（過学習）しにくくなることも知られている。 

## [CNN: 畳み込みニューラルネットワーク](https://ja.wikipedia.org/wiki/%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)

![](https://cdn-images-1.medium.com/freeze/max/1000/1*UgbuN6hIr34GOo_5Gzb-DA.png?q=20
)
https://mc.ai/deep-learning%E2%80%8A-%E2%80%8Aconvolutional-neural-networks-cnn/

特に画像認識に応答するために改良されたディープニューラルネットワーク。
自動運転技術など、非常に広く応用されている。

従来のシグモイド関数のような活性化関数では、層が深くなるにつれ勾配が小さくなってくのでうまく学習ができないので、勾配消失問題を解決した ReLU関数を用いる。

### 処理の流れ

1. 畳み込み層  

  フィルタを用いて積和演算 + 活性化関数の作用を行う層。
  元の画像の特徴が抽出された小さな画像である「特徴マップ」に変換される。

  出力画像のサイズを調整するために元の画像の周りを固定の値で埋める「パディング」を行う。

2. プーリング層  

  畳み込み層から「特徴マップ」を受け取り、平均値、最大値を用いてサブサンプリングを行う層。
  平均プーリング、最大プーリングによって更に小さな画像に変換される。

3. 全結合層  

  プーリング層から出力された画像データを、縦横に並んだ2次元データから、一列に並べたフラットな1次元データに変換する出力層。

### 手法

##### LeNet
1998年にヤン・ルカンらによって提案された、手書き数字認識を行うネットワーク。

シグモイド関数を使用して、サンプリングによって中間データのサイズ縮小を行っている。
初めての CNN。

### 畳み込み層

畳み込みフィルタと呼ばれる、画像中の特定の形状に反応するフィルタを画像に掛けけあわせる処理が行われる。
これらのフィルタは、学習によってラベルの判別に有効な形状になっている。

画像内に畳み込みフィルタを一致するような部分があれば、その部分が強調されて移されて、「特徴マップ」と呼ばれる画像を生成する。

### プーリング層

あるサイズのウィンドウを画像のすべての部分にあてはめ、そのウィンドウの中から１つだけ値を抽出して新しい画像に写す操舵をする。

##### マックスプーリング（Max Pooling）
ウィンドウの中でもっとも大きい値を抽出する。配列が小さくなるので、データ量を削減できる。

多くの CNN モデルで利用されている。

##### avgプーリング
ウィンドウの中の値の平均値を抽出する。

### 全結合層
畳み込み層、プーリング層の処理で得られた特徴マップを読み込み、これらに含まれる特徴量を抽出する。
最終的には、出力層に予測や分類の結果を出力する。

畳み込み処理と同様に、何層にも積み重ねることでより複雑かつ有効な特徴量を利用した処理が可能。

通常のニューラルネットワークと同じ構造を取る。

##### データ拡張
様々なパターンを網羅したデータを収集する代わりに、手元のデータから擬似的に別のデータを生成する手法。データの水増しとも言われる。

手元にあるデータそれぞれに対して、ランダムにいくつかの処理を施して新しいデータを作り出す。

- 上下左右にずらす
- 上下左右を反転する
- 拡大・縮小する
- 回転する
- 斜めに歪める
- 一部を切り取る
- コントラストを変える

### CNN の発展型

##### AlexNet
2012年の ILSVRC でトロント大学のジェフリー・ヒントン率いるチームが用いた手法。 

- 活性化関数に ReLU を使用
- LRN (Local Response Noralization) という、局所的正規化を行う層を用いる
- ドロップアウトを使用する

##### VGG
畳み込み層とプーリング層から構成される基本的な CNN。画像の分類タスクで用いられる。

重みのある層を全部で16層まで重ねてディープにしている。
3x3 の小さなフィルターによる畳み込み層を連続して行っている点が特徴。

##### GoogLeNet
基本的には CNN と同じ構成で、ネットワークが縦方向の深さだけでなく、横方向にも深さ（広がり）を持っているのが特徴。
横方向の幅は「インセプション構造」と呼ばえる。

##### Skip connection
層を飛び越えた結合

## RNN: リカレントニューラルネットワーク

![](https://miro.medium.com/max/1400/1*aIT6tmnk3qHpStkOX3gGcQ.png)
https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce

時系列データにおいて、ある時間的に近接した要素同士は影響を与え合う可能性が高いが、時間的に遠く離れた要素が影響を与えることは少ない。
この性質を使えば、パラメータの数を減らすことができる。これが RNN である。

時系列データを入力して、データから時間依存性を学習できるモデル。
内部に閉路（行って戻ってくる経路）を持つニューラルネットワーク。
RNN は過去の情報を保持できるため、過去の入力を参考に「時系列データの次の時点での値」を予測する。
自然言語処理への応用が盛んで、機械翻訳技術などに応用されている。

##### 入力重み衝突

##### 出力重み衝突

##### BPTT (Backpropagation Through Time)
RNN を順伝播型ネットワークに置き換えて誤差逆伝播法を適用する手法。

##### CTC
入出力間で系列長が違う場合のニューラルネットワークを用いた分類法。

##### [LSTM (Long Short-Term Memory)](https://ja.wikipedia.org/wiki/%E9%95%B7%E3%83%BB%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6)
遠い過去の入力を現在の出力に反映する手法。

従来の RNN の欠点であった「短期的な情報しか保持できない」という点を改良し、長期的な情報を考慮して予測計算を行えるようにしたもの。

##### seq2seq
時系列データを入力・処理し、時系列データを出力するモデル。

代表的な応用として入力と出力を異なる言語列とする翻訳が実現できる。
これは NMT（ニューラル機械翻訳）と呼ばれ、従来の SMT（統計的機械翻訳）よりも大幅に性能が向上している。

##### GRU (Gated Recurrent Unit)

##### Bidirectional RNN
LSTM を2つ（未来と過去）組み合わせることで、未来から過去方向も含めて学習できるモデル。

##### Sequence-to-Sequence
入力が時系列なら出力も時系列で予測する。自然言語処理を中心に研究されている分野。

##### RNN Encoder–Decoder
2つの LSTM　を組み合わせて、時系列を入力して時系列を出力する手法。（Sequence-to-Sequence）

モデルは大きく「エンコーダ」と「デコーダ」の2つの LSTM に分かれており、エンコーダが入力データを、デコーダが出力データをそれぞれ処理する。

##### Attention
過去の時点それぞれの重みを学習することで、時間の重みをネットワークに組み込む。

## 転移学習

学習済みのネットワークを利用して新しいタスクの識別に利用する手法。

### 学習の方法

##### 特徴抽出
ディープニューラルネットワークの最後の層では、分類に有効な特徴量が伝わっていると考えられるため、最後の層を切り取って、代わりに SVM などの従来の機械学習分類器をつなげても、うまく分類できる。

##### ファインチューニング
学習済みのモデルを使い、最後の方の層だけをタスクに合わせて学習し直す（微調整する）手法。

最初の方の層で捉えられる特徴はタスクによって変わることはないが、最後の層で捉えられる特徴はタスクによって変わるため、学習をやり直す必要がある。

### 課題と関連分野

##### ドメイン適合
ドメインを変えて転移学習を行う手法。
（ex. 新聞記事を分析するように学習したモデルを初期値にして、ツイートを分析するモデルを作成する）

##### ドメイン混同
通常出力する値の他に、入力したデータのドメインも出力することで、モデルが特定の領域に特化しないようにする手法。

##### 負の転移
転移元と転移先のタスクがそれほど似ていないので、通常の学習よりも性能の悪いモデルが作られてしまうこと。

##### マルチタスク学習
複数のタスクについて同時に学習させること。通常の転移学習は一度に特定のタスクについてモデルを学習させる。

##### One-shot 学習
あるラベルについての訓練データが一つ（あるいは少数）しかなくても正しい出力ができる手法。
「一を聞いて十を知る」学習方法。

特定のラベルがついた訓練データが存在しない場合にそのラベルを出力するような、「Zero-shot 学習」と呼ばれる手法も研究されている。

## 深層強化学習

強化学習は、システムが「状態」を定義し、試行錯誤を重ねながら、自動的に報酬を最大化する「方策」を見つけることができる。
しかし、こうした問題はかなり厄介で、1990年代には活発に研究が行われたものの、2000年代に入るとその勢いも衰えてしまいました。

ディープラーニングの登場でその状況が打開されつつあり、さらに探索による先読みを組み合わせることで、AlphaGo（碁）の躍進につながった。

##### 強化学習
行動を学習する仕組み。

ある環境で、目的とする報酬（スコア）を最大化するためにはどのような行動をとっていけばいいかを学習していく。
機械学習では、想定する「状態」の数が非常に多くなったり、人による「状態」の想定に限界があったりした。

強化学習のアルゴリズムは、「モデルベース」と「モデルフリー」に大別される。

##### Q学習
価値ベースの方法で、「ある状態においてある行動をとることがどれくらい良いか」という行動の価値を行動していく。


### 改善モデル

強化学習の改善手法としては、以下の3種類に基づいている。

##### 方策ベース

##### 状態価値関数（Q関数）ベース

##### モデルベース

##### RAINBOWモデル
上記3つのモデルを入れた手法。

##### [BRETT](https://engineering.berkeley.edu/brett/)
カリフォルニア大学バークレー校が開発している、ディーブラーニングと強化学習を組み合わせたロボット。

報酬の設定を変更すれば、異なる動作を学習することができる。
一旦学習した内容はコピーして、同じタイプのロボットであれば同じように動かすことができる。

##### [AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo)
DeepMind社が開発した碁のプログラム。  
どのような手を打つべきかの探索には[モンテカルロ木探索](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%9C%A8%E6%8E%A2%E7%B4%A2)が用いられている。

##### [AlphaGo Zero](https://ja.wikipedia.org/wiki/AlphaGo_Zero)
2017年10月に発表された、棋譜を全く必要としない、完全に自己対局のみで学習していく碁のプログラム。
従来の AlphaGo を超える強さとなった。

##### [DQN (Deep Q-Network)](https://ja.wikipedia.org/wiki/DQN_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF))
DeepMind が考案した、Q学習の行動価値関数を、深い構造を持ったニューラルネットワークで置き換えたモデル。

アタリ社のゲームを学習された例では、49ゲームのうち29ゲームで、人間並、あるいはそれ以上のスコアを出した。

### [OpenAI Gym](https://gym.openai.com/)

強化学習のシュミレーション用プラットフォーム。

公式ドキュメントでさまざまなチュートリアルが用意されているので、実際に動かして強化学習の面白さを体験できる。

[![Image from Gyazo](https://i.gyazo.com/80b3e3b6c140c6a5767a8a07904a450f.gif)](https://gyazo.com/80b3e3b6c140c6a5767a8a07904a450f)
Link: [OpenAI Gym](https://gym.openai.com/)

## 深層生成モデル

ディーブラーニング技術は、識別や回帰だけでなく、これまで存在していなかったデータ、例えば架空の画像の生成にも利用できる。

一般に「生成モデル」と呼ばれるモデルを使うことで、AI 技術による「創作」が可能となる。
ディープニューラルネットワークを使った生成モデルは「深層生成モデル」と呼ばれる。

##### WaveNet

##### 生成モデル

##### [VAE (変分オートエンコーダ)](https://ja.wikipedia.org/wiki/%E5%A4%89%E5%88%86%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80%E3%83%BC)
オートエンコーダの中間層の圧縮された特徴表現の数値を確率分布に従うように変更し、さらにエンコーダの出力とデコーダーの入力をこれにあわせて変更したのもの。

確率分布に従うことで、圧縮された特徴表現の数値を変動させ、デコーダーの所期の性能（いろいろな猫の画像の生成）を達成できる。

##### [GAN (敵対的生成ネットワーク)](https://ja.wikipedia.org/wiki/%E6%95%B5%E5%AF%BE%E7%9A%84%E7%94%9F%E6%88%90%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)
2014年に[イアン・グッドフェロー](https://ja.wikipedia.org/wiki/%E3%82%A4%E3%82%A2%E3%83%B3%E3%83%BB%E3%82%B0%E3%83%83%E3%83%89%E3%83%95%E3%82%A7%E3%83%AD%E3%83%BC)らによって発表された、生成ネットワークと識別ネットワークからなる教師なし学習法。

[ヤン・ルカン](https://en.wikipedia.org/wiki/Yann_LeCun)が GAN について、「機械学習において、この10年間で最もおもしろいアイディア」であると形容した。

トレーニングに利用したデータに類似した出力を生成できるため、希少データの水増しや、作風をまねた絵画や楽曲の生成など、様々な応用が進められている。

- 生成ネットワーク  

  ランダムノイズベクトルから入力データのレプリカを作ろうとする
  訓練データと同じようなデータを生成する

- 識別ネットワーク  

  そのデータが訓練データから来たものか、生成ネットワークから来たものかを識別する

##### DCGAN (Deep Convolutional GAN)

## GNN: グラフニューラルネットワーク

グラフ構造（データ間のつながりを持った構造）のデータを入力とするニューラルネットワーク。

GNN やオートエンコーダ、RNN を構築することができるため、広範囲の応用が可能。
分子構造の予測や自然言語処理などへの応用がある。

## 深層信念ネットワーク
2006年にジェフリー・ヒントンが提唱した、教師なし学習（オートエンコーダに相当する層）に制限付きボルツマンマシンという手法を用いている。

##### [ボルツマンマシン](https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AB%E3%83%84%E3%83%9E%E3%83%B3%E3%83%9E%E3%82%B7%E3%83%B3)
1985年にジェフリー・ヒントンとテリー・セジュノスキーによって開発された、確率的回帰結合型ニューラルネットワークの一種。


## 用語

##### [ノーフリーランチ定理](ノーフリーランチ定理)
「あらゆる問題に対して万能なアルゴリズムは存在しない」という定理。

##### 単純パーセプトロン
1958年にアメリカの心理学者[フランク・ローゼンブラット](https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%A9%E3%83%B3%E3%82%AF%E3%83%BB%E3%83%AD%E3%83%BC%E3%82%BC%E3%83%B3%E3%83%96%E3%83%A9%E3%83%83%E3%83%88)が提案したニューラルネットワークの元祖のモデル。

##### 多層パーセプトロン

##### スパース性
物事の本質的な特徴を決定づける要素はわずかであるという性質。この性質を利用した「[スパースモデリング](https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%91%E3%83%BC%E3%82%B9%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0)」という手法もある

成分のほとんどが 0 である行列のことを「[スパース行列](https://ja.wikipedia.org/wiki/%E7%96%8E%E8%A1%8C%E5%88%97)」とも言う

##### DistBelief
Googleが開発した深層分散学習のフレームワーク。

分散並列処理技術で高速な処理が可能となっている。

## データセット

こちらのドキュメントに機械学習で利用可能なデータセットがまとまっている。

https://github.com/arXivTimes/arXivTimes/tree/master/datasets

##### MNIST（エムニスト）
手書き文字認識学習用データ。

手書き数字画像60,000枚と、テスト画像10,000枚を集めた、画像データセット。
さらに、手書きの数字「0〜9」に正解ラベルが与えられるデータセットでもあり、画像分類問題で人気の高いデータセット。

URL: http://yann.lecun.com/exdb/mnist/

##### Imagenet
スタンフォード大学がインターネット上から画像を集め分類したデータセット。

一般画像認識用に用いられる。ImageNetを利用して画像検出・識別精度を競うThe ImageNet Large Scale Visual Recognition Challenge（ILSVRC）などコンテストも開かれている。

URL: http://www.image-net.org/